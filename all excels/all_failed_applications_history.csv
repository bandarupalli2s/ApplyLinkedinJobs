Job ID,Job Link,Resume Tried,Date listed,Date Tried,Assumed Reason,Stack Trace,External Job link,Screenshot Name
4260751210,https://www.linkedin.com/jobs/view/4260751210,Pending,Unknown,2025-07-09 00:07:19.192123,Required experience is high,"
About the job
Our client is currently seeking a Senior DevOps Engineer 

Overview
In this role, you will be responsible for the design, development, and deployment of scalable and secure software solutions across the full software development lifecycle. You will work closely with both front-end and back-end technologies, with a strong focus on Java, Spring Boot, Azure (AKS), and DevOps practices.
You will contribute to building robust microservices, integrating cloud-native solutions, and ensuring continuous integration and delivery pipelines are efficient and secure. This role follows Agile development methodologies and requires strong collaboration and problem-solving skills.
Minimum Qualifications
8â€“10 years of experience in software development and DevOps engineering.
Proficiency in Java (1.8, 21) and Spring Boot frameworks.
Experience with cloud platforms, especially Azure and Azure Kubernetes Service (AKS).
Strong understanding of DevOps principles, CI/CD pipelines, and infrastructure automation.
Hands-on experience with Docker, GitHub, and Gradle/Maven.
Familiarity with Unix/Linux environments and shell scripting.
Experience with SQL and NoSQL databases (e.g., Oracle, Cosmos DB, MarkLogic).
Knowledge of microservices architecture and cloud-native design patterns.
Preferred Qualifications
Experience with Azure DataBricks, Azure Data Factory (ADF).
Familiarity with Kafka, Python, and JUnit for testing.
Exposure to Spring Integration and Spring Batch.
Experience with PCF (Pivotal Cloud Foundry) or similar cloud platforms.
Proficiency with development tools such as IntelliJ or Eclipse.
Rate: $70-80/HR

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4260761965,https://www.linkedin.com/jobs/view/4260761965,resume.pdf,Unknown,2025-07-09 00:08:36.145739,Problem in Easy Applying,"Message: stale element reference: stale element not found in the current frame
  (Session info: chrome=138.0.7204.94); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception
Stacktrace:
0   chromedriver                        0x0000000104db4e6c cxxbridge1$str$ptr + 2722840
1   chromedriver                        0x0000000104dacd74 cxxbridge1$str$ptr + 2689824
2   chromedriver                        0x00000001048fe3ec cxxbridge1$string$len + 90648
3   chromedriver                        0x00000001049040a0 cxxbridge1$string$len + 114380
4   chromedriver                        0x00000001049063e4 cxxbridge1$string$len + 123408
5   chromedriver                        0x000000010490648c cxxbridge1$string$len + 123576
6   chromedriver                        0x0000000104945114 cxxbridge1$string$len + 380736
7   chromedriver                        0x000000010493ade4 cxxbridge1$string$len + 338960
8   chromedriver                        0x0000000104986934 cxxbridge1$string$len + 649056
9   chromedriver                        0x0000000104939834 cxxbridge1$string$len + 333408
10  chromedriver                        0x0000000104d77f88 cxxbridge1$str$ptr + 2473268
11  chromedriver                        0x0000000104d7b1f4 cxxbridge1$str$ptr + 2486176
12  chromedriver                        0x0000000104d599d0 cxxbridge1$str$ptr + 2348924
13  chromedriver                        0x0000000104d7bab0 cxxbridge1$str$ptr + 2488412
14  chromedriver                        0x0000000104d4aa60 cxxbridge1$str$ptr + 2287628
15  chromedriver                        0x0000000104d9b9a0 cxxbridge1$str$ptr + 2619212
16  chromedriver                        0x0000000104d9bb2c cxxbridge1$str$ptr + 2619608
17  chromedriver                        0x0000000104dac9b0 cxxbridge1$str$ptr + 2688860
18  libsystem_pthread.dylib             0x0000000198dc2c0c _pthread_start + 136
19  libsystem_pthread.dylib             0x0000000198dbdb80 thread_start + 8
",Easy Applied,Not Available
4260751210,https://www.linkedin.com/jobs/view/4260751210,Pending,Unknown,2025-07-09 00:17:07.607861,Required experience is high,"
About the job
Our client is currently seeking a Senior DevOps Engineer 

Overview
In this role, you will be responsible for the design, development, and deployment of scalable and secure software solutions across the full software development lifecycle. You will work closely with both front-end and back-end technologies, with a strong focus on Java, Spring Boot, Azure (AKS), and DevOps practices.
You will contribute to building robust microservices, integrating cloud-native solutions, and ensuring continuous integration and delivery pipelines are efficient and secure. This role follows Agile development methodologies and requires strong collaboration and problem-solving skills.
Minimum Qualifications
8â€“10 years of experience in software development and DevOps engineering.
Proficiency in Java (1.8, 21) and Spring Boot frameworks.
Experience with cloud platforms, especially Azure and Azure Kubernetes Service (AKS).
Strong understanding of DevOps principles, CI/CD pipelines, and infrastructure automation.
Hands-on experience with Docker, GitHub, and Gradle/Maven.
Familiarity with Unix/Linux environments and shell scripting.
Experience with SQL and NoSQL databases (e.g., Oracle, Cosmos DB, MarkLogic).
Knowledge of microservices architecture and cloud-native design patterns.
Preferred Qualifications
Experience with Azure DataBricks, Azure Data Factory (ADF).
Familiarity with Kafka, Python, and JUnit for testing.
Exposure to Spring Integration and Spring Batch.
Experience with PCF (Pivotal Cloud Foundry) or similar cloud platforms.
Proficiency with development tools such as IntelliJ or Eclipse.
Rate: $70-80/HR

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4262821900,https://www.linkedin.com/jobs/view/4262821900,Pending,Unknown,2025-07-09 00:17:35.619928,Asking for Security clearance,"
About the job
General Summary Of Position
Granite has an immediate need for a mid-level Cloud DevOps Engineer to support our ongoing automation and cloud modernization initiative. This role is ideal for a DevOps enthusiast with a solid understanding of cloud computing and passion for streamlining development and deployment processes.
The right candidate will be a self starter, able to work independently or as a team member. Must be able to thrive in a fast-paced environment and learn new technologies quickly. This is a growing company where you will be able to have a significant impact on our internal processes and get a chance to add directly to the goals of the organization.
Duties And Responsibilities
Build, release and maintain CI/CD pipelines to support application development and deployment workflows
Define and document DevOps processes, standards, and tooling in a greenfield cloud environment
Implement and manage Infrastructure as Code (laC) using tools like Terraform
Set up monitoring, logging, and alerting frameworks (e.g., Elastic, Prometheus, Grafana)
Champion automation and self-service wherever possible to improve Developer experience and reduce manual intervention
Collaborate with development teams to optimize build, test, and release pipelines
Integrate DevSecOps practices by embedding automated security and compliance checks (e.g., vulnerability scanning, static code analysis, and secrets management) into CI/CD pipelines
Design and manage cloud IAM policies and roles to enforce least-privilege access, automate user and service account provisioning, and maintain secure identity controls across environments
Assist in defining, designing and implementing cloud strategy and best practices
Lead and support incident response and root cause analysis for cloud infrastructure issues
Mentor junior engineers and help grow a culture of operational excellence 
Required Qualifications
Excellent scripting skills (Bash, Python. Go, or similar)
4+ years of experience in Cloud DevOps and / or Cloud Engineering
Hands-on experience and knowledge of GCP, AWS, OCI or Azure - compute, storage and network operation concepts
2+ years of hands-on experience with Google Cloud Platform (GCP) services such as GKE, Cloud Run, Cloud Functions, Pub/Sub, IAM, and vpc
Strong skills in Terraform, Pulumi, or similar lac frameworks
Proven experience building DevOps pipelines using Azure DevOps, GitHub Actions, Jenkins, or similar tools Proficient in containerization and orchestration (Docker, Kubernetes/GKE)
Solid understanding of networking, security principles, and access control in cloud environments
Experience leading projects in greenfield or early-stage cloud environments
Preferred Qualifications
Familiarity with security controls / concerns around enterprise software, DevSecOps
Certification(s) with Cloud platforms (GCP, Azure or AWS)
Experience with multi-cloud and hybrid environments
Experience with building and managing Al/MLOPs pipelines and infrastructure for Al workloads
Experience with on premises to cloud (public or private) transformation
Experience in Agile Framework (SAFe) and Agile development methodology
Experience with test automation tools and integrating NUnit or JUnit tests
Experience with client server architecture, service-oriented, microservice architecture Experience with Windows and .NET environments

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4260717006,https://www.linkedin.com/jobs/view/4260717006,Pending,Unknown,2025-07-09 00:18:18.892323,Found a Bad Word in About Job,"
About the job
Job Role: OpenShift Administrator (with Linux administration, Automation & DevOps Expertise)
Work Locations:(Pittsburgh-PA/Strongsville-OH/Birmingham-AL/Dallas-TX/Phoenix, AZ)

IMMEDIATE HIRING

Job Summary:

Clarium is seeking a skilled OpenShift Administrator with a strong background in Linux system administration, scripting, automation and DevOps practices. The ideal candidate will be responsible for maintaining and supporting RedHat OpenShift clusters in production environment, providing 24*7 support, incident triage, and participating in release activities. You will work closely with cross-functional teams to ensure system reliability, scalability, and performance.

 
Key Responsibilities:
  Administer and maintain RedHat OpenShift clusters in production and non-production environments. Upgrade Clusters
Perform Linux system administration, troubleshooting, patching
Monitor cluster health, manage node scalability, resource optimization and ensure cluster availability (No downtime)
Troubleshoot and resolve issues related to infrastructure, network, pods, containers, and services
Participate in 24x7 production support (on-call rotation), handle critical incidents, and ensure minimal platform downtime
Develop and maintain shell/python scripts for automation of routine tasks and monitoring
Implement and support CI/CD pipelines using tools like Jenkins, GitOps Approach, or ArgoCD
Collaborate with development and release teams to support deployment and release activities
Create and maintain documentation for systems, processes, and standard operating procedures (SOPs)
Ensure compliance with security, backup, and disaster recovery policies
Platform monitoring using Dynatrace


Required Qualification:

â€¢ 5+ years of hands-on experience with RedHat OpenShift (v4.x preferred).
â€¢ Strong proficiency in Linux (RedHat Linux, CoreOS) system administration.
â€¢ Solid experience in Shell scripting, bash, and/or Python.
â€¢ Knowledge of DevOps tools such as Jenkins, Git, Ansible, Helm, Docker, and GitOps workflows.
â€¢ Understanding of container orchestration, pod scheduling, and cluster level debugging.
â€¢ Experience working in production support environments with on-call responsibilities
â€¢ Familiarity with monitoring and logging solutions such as Prometheus, Grafana, ELK, FluentD.
â€¢ Good problem-solving skills and experience in incident triage and root cause analysis

 Preferred Qualifications:

â€¢ RedHat Certified Specialist in OpenShift Administration or equivalent
â€¢ Certified Kubernetes Administration (CKA)
â€¢ Knowledge of Kubernetes, Istio, or service mesh technologies
â€¢ Experience with Azure cloud platform
â€¢ Experience with ArgoCD workflows


Work Authorization:

Must be authorized to work in the United States. (US Citizen/GC/H1B/GC EAD)

Work Environment:

Fast-paced, enterprise-grade infrastructure with high uptime requirements
Expectation to work during critical incidents, maintenance windows, and release weekends.
Team collaboration across IT operations, development, QA, and DevOps

Compensation & Benefits:

Competitive salary based on experience.
Medical, dental, vision insurance.
401(k) with company match.
Paid time off and holidays.
Professional development opportunities.

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4262178052,https://www.linkedin.com/jobs/view/4262178052,Pending,Unknown,2025-07-09 00:18:21.353884,Found a Bad Word in About Job,"
About the job
Position: Senior Site Reliability Engineer - No C2C
Location: 100% Remote in USA
Duration: Initial 6-month contract, with potential extensions

Overview:
Seeking a Senior Site Reliability Engineer (SRE) to support the development and transformation of its cloud-native technology stack. This role will encompass the full software, data, and infrastructure lifecycle, contributing to the reliability, scalability, and performance of systems. The ideal candidate will have a robust background in financial services, strong leadership abilities, and experience managing platforms that require continuous operation on a 24x6 basis.

Key Responsibilities:
Lead the implementation and transformation of cloud-native technology stack across all SDLC phases.
Ensure the operational continuity and reliability of systems, meeting high availability requirements.
Collaborate across teams to design, deploy, and maintain scalable solutions.
Leverage best practices to manage complex systems and infrastructure in a cloud-native environment.
Provide expertise in building and maintaining resilient, automated platforms.
Monitor system health, resolve incidents, and improve system performance.
Provide leadership in driving platform stability and enhancing operational efficiency.

Qualifications:
Proven experience in financial services or related industries.
Strong leadership and team management skills.
Experience with cloud-native architectures, automation, and infrastructure management.
In-depth knowledge of Site Reliability Engineering practices and tools.
Ability to manage mission-critical platforms with a focus on continuity and performance.
Excellent problem-solving and troubleshooting skills.

Additional Information:
This is a remote position.
The project duration is initially six months, with the possibility of extensions.

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4256162359,https://www.linkedin.com/jobs/view/4256162359,Pending,Unknown,2025-07-09 00:18:25.714359,Required experience is high,"
About the job
Senior DevOps Engineer

Position Overview
We are seeking a dynamic individual to fill the role of Senior DevOps Engineer, someone who not only embraces challenges but thrives in proactively solving them. If you are a self-starter with a passion for unraveling the root of issues, automating solutions, and approaching problems from a holistic perspective, this role is for you. In this role, you will be able to help design and implement the backbone of development automation.

Key Responsibilities
Expertise in Kubernetes
Proactive Problem Solver: Identify and address the root causes of issues, focusing on solving problem categories rather than individual instances. Engage early and comprehensively.
Automation Enthusiast: Leverage automation tools such as build and deployment pipelines, Docker, Python, Bash, and Ansible to streamline and eliminate recurring issues.
Multifaceted Expertise: Proficient in CI/CD automation, SDLC, Cloud computing (AWS), Python, Bash, and Ansible, demonstrating versatility across a range of technologies.
Comprehensive Problem Analysis: Conduct thorough investigations to understand all facets of a problem, ensuring comprehensive and effective solutions.
Translate complex problems into manageable, actionable chunks that can be implemented and iteratively improved.
Documentation Excellence: Demonstrate exceptional communication and documentation skills, ensuring clear and comprehensive records for processes, solutions, and potential improvements.
Culture: Strong focus on helping to establish and maintain a DevOps culture, setting development teams up for success by enabling and encouraging self sufficiency

Knowledge, Skills and Experience Required
Bachelorâ€™s degree from an accredited college or university with specialization in an information technology field, equivalent combination of related education and/or work experience totaling a minimum of 8+ years
Expertise in using git and SDLC
Competency in using Linux/UNIX systems
Competency in Cloud Computing, preferably AWS
High proficiency in CI/CD automation workflows, preferably Gitlab
Experience and competency in using containerized systems
Working knowledge of TCP/IP networking
Comfort with utilizing metrics or monitoring solutions for troubleshooting systemic issues
Experience with REST APIâ€™s and webservice solutions is desirable
Ability to perform code reviews in pipeline code (yaml), Docker, Ansible, Python, and Bash
Ability to flexibly adapt to a rapidly changing environment and generate effective and innovative solutions to address change.
Proven record of excellent business judgment and acumen.
Outstanding and persuasive oral and written communication skills that effectively reach all levels of the organization.
Proven ability to prioritize, reprioritize and demonstrates appropriate agility to manage competing and sometimes conflicting priorities.
Demonstrated people leader with experience working on remote teams, including technical mentorship
Experience advising leadership on enhancements and best practices for CI/CD
Strong communication skills with ability to influence at all levels of the organization; ability to simplify complex topics for consumption and critical decision making

Experience required 8 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4264250788,https://www.linkedin.com/jobs/view/4264250788,Pending,Unknown,2025-07-09 00:18:28.311319,Asking for Security clearance,"
About the job
****************Only USC / Green Card/ GC EAD/H4 EAD****************


Position: Sr. DevOps Consultant
Location: Pennington, NJ/Jersey City (Hybrid model) - 3 days onsite
Duration: 18 months
Client: Bank of America
Rate: $70-73/hr on W2. 


Responsible for defining and delivering automated credential management solutions aligning with the enterprise strategy.
Ensures the solution is fit for purpose by working with internal stakeholders as well as external subject matter experts to meet the enterprise needs for secret and credential management.
Consults with business stakeholders to clearly understand the business requirements, challenges of the solution, and finds creative solutions to meet the requirements.
Clarifies the architecture via detailed technical documentation.
Performs design and code reviews to ensure all functional requirements for a solution are sufficiently met

Responsibilities:
As our DevOps engineer, play a crucial role in bridging the gap between development and operations.
Collaborate with cross-functional teams to automate deployment processes and monitoring of infrastructure
Automate the next generation of Privilege Access Vaulting solution with a focus on high-availability and resiliency
Develop and maintain automated solutions leveraging Terraform, Jenkins, Ansible, Git
Managing and optimizing cloud infrastructure on platforms like Azure and AWS
Conducting proactive monitoring and troubleshooting of all systems
Maintaining up-to-date documentation on processes and configurations

Required Qualifications
7+ years of DevOps experience
Strong experience in Ansible, Jenkins, Terraform
Experience taking manual tasks/processes and automated at the enterprise level.
Demonstrated expertise of scripting via common solutions such as Python and PowerShell.
Exposure to working in environment that is heavy on operations technology (automation, optimization, rapid development)
Demonstrated expert level of RHEL Linux OS and LDAP Directory administration.
Solid understanding and demonstrable expertise delivering Linux based automated solutions at enterprise scale.
Solid understanding of infrastructure (hardware, network, storage).
Good understanding of agile methodology and associated toolset (Jira, Bitbucket, Jenkins, etc.)

Desired Qualifications
Previous experience with credential vaulting products such as: CyberArk, Centrify, HashiCorp Secrets Vault.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4264126298,https://www.linkedin.com/jobs/view/4264126298,Pending,Unknown,2025-07-09 00:18:36.712319,Found a Bad Word in About Job,"
About the job
ðŸŒŸ Principal DevOps Engineer with AI 

Full-Time | Hybrid (3 Days Onsite â€“ New Jersey) | No Visa Sponsorship

Join a Cutting-Edge AI & DevOps Innovation Team
We're seeking a Principal DevOps Engineering Specialist with AI to play a pivotal role in advancing Large Language Model (LLM) capabilities within a modern DevOps ecosystem. This hybrid, full-time position (3 days onsite in New Jersey) is ideal for a hands-on, highly skilled engineer passionate about GenAI, automation, and AI-driven development practices.

ðŸ” What You'll Do
As a core member of the engineering team, you'll:
Drive enhancements to our DevOps processes including automation of requirements gathering, code generation, and test creation.
Design, implement, and refine LLM-based prompts to guide GenAI outputs that are accurate, relevant, and secure.
Contribute to the development of internal tools and frameworks that support software delivery and improve team productivity.
Work collaboratively across cross-functional development teams and third-party vendors to ensure smooth integration and scalable systems.
Stay ahead of the curve by exploring new GenAI techniques, tools, and models to drive innovation.
Lead initiatives in documentation, automation testing, and process reliability.

ðŸ’¡ Tech Youâ€™ll Work With
LLMs (Multiple models)
Microsoft Azure & Azure DevOps Server
GitHub Copilot
Bash, Ansible (Automation tools)
CI/CD, Infrastructure as Code, DevOps frameworks
Service-Oriented Architectures (SOA)

âœ… Your Background
Proven experience improving DevOps pipelines and implementing automation strategies.
Hands-on experience designing prompts and workflows using Generative AI / LLMs.
Knowledge of cloud platforms, especially Azure.
Familiarity with modern software development methodologies and scripting tools.
Strong collaboration and documentation skills.
At least 4â€“6 years of relevant experience and a BA/BS in Computer Science or related field.
2+ years working with LLMs in a development or automation context.

ðŸš« Please Note
No Visa Sponsorship Available â€“ You must be authorized to work in the U.S.
No C2C / No Third-Party Vendors

ðŸ¢ About Hirobe Limited
Expert recruitment, powered by a deep understanding of the Adobe Experience Cloud.
Since 2018, we've been connecting the best Adobe professionals with leading organizations, giving us a unique insight into both the technology and the talent. What truly sets us apart is our specialized market knowledge, tailored service, and unwavering commitment to finding the ideal fit for everyone involved.

ðŸ“¬ Ready to Apply?
This is an urgent hire!
ðŸ“§ Send your resume to: raj@hirobe.io
ðŸ“ž Or call: +1 917-512-8959

Letâ€™s redefine whatâ€™s possible with AI and automationâ€”together.

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4258335815,https://www.linkedin.com/jobs/view/4258335815,Pending,Unknown,2025-07-09 00:19:02.340424,Required experience is high,"
About the job
Opportunity is located in the Boca Raton Headquarters onsite 5 days a week.


FinTech, eCommerce company, that provides brand name durable goods to consumers on a lease-to-own (LTO) basis through its ecommerce marketplace and LTO payment method. It also provides LTO technology platforms to retailers and e-tailers to enter transactions with consumers who want to obtain durable goods, but do not have the available cash or credit.

Sponsorship Notice: Applicants must be authorized to work in the United States without the need for current or future visa sponsorship.

We enjoy an accessible fast-paced onsite work environment, where we collaborate with colleagues at all levels of the organization.

FlexShopper is looking for an experienced DevOps Engineer to design and implement cloud-based infrastructure using AWS. This role focuses on scalable architecture design and secure integration with current applications. Key tasks include deploying containerized solutions with Kubernetes and supporting continuous integration and deployment.

Key Responsibilities:
Design enterprise infrastructure for cloud computing, focusing on SaaS, PaaS, and IaaS.
Implement containerized architecture using Kubernetes and support AWS services.
Manage continuous integration, deployment, and configuration management.
Optimize and troubleshoot infrastructure performance, ensuring high availability.
Requirements:
7-10 years experience
Strong experience with AWS, Kubernetes, and container orchestration.
Proficiency in scripting (Bash, Ruby, Python, or Node).
Familiarity with GitOps (Flux or Argo), ELK, and microservice architecture.
Experience with CI/CD tools and Agile methodologies.
Preferred:
Experience scaling MySQL/MongoDB, Node.js, Redis, RMQ, Kafka.
Knowledge of Jenkins, Deis, Rancher, CoreOS, or Mesosphere.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4252559741,https://www.linkedin.com/jobs/view/4252559741,Pending,Unknown,2025-07-09 00:19:04.717460,Required experience is high,"
About the job
Weâ€™re hiring experienced Azure Senior DevOps Engineers to join our client's high-impact team driving product-level transformation initiatives. This is a full-time, permanent position requiring onsite work. Weâ€™re looking for individuals who can work independently, think architecturally, and take full ownership of DevOps delivery across cloud infrastructure.

Responsibilities:
Build and integrate robust Azure DevOps pipelines
Provision infrastructure using Terraform
Support and scale Azure IaaS environments
Identify and correct issues in AI-generated code
Collaborate across architecture, engineering, and infrastructure domains
 Qualifications:
7â€“10 years of DevOps experience
2â€“4 years of hands-on experience with Azure Cloud and IaaS
Strong background in infrastructure, architecture, and scripting
Preference for candidates who began in systems administration and evolved into DevOps
Self-directed mindset with a proactive approach to problem-solving
 Why Join:
Youâ€™ll play a central role in a forward-looking technology environment, contributing to strategic transformations with direct business impact. If youâ€™re ready to take the next step in your DevOps career, apply now or message us with questions. Weâ€™d love to connect.

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4264158371,https://www.linkedin.com/jobs/view/4264158371,Pending,Unknown,2025-07-09 00:19:55.308019,Found a Bad Word in About Job,"
About the job
Title:  Site Reliability Engineer
Location: Miami, FL
Duration: 6+ months
Compensation: $55.00 -60.00
Work Requirements: US Citizen, GC Holders or Authorized to Work in the U.S.
 
Site Reliability Engineer

Description: 
Consultant will play a critical role in ensuring the reliability, performance, and seamless operation of our digital ecosystem. This includes our guest-facing mobile apps, websites, and the backend systems that power them. You will work collaboratively with development, operations, and product teams to build and maintain a highly resilient and scalable digital experience for our guests.

Essential Duties and Responsibilities: 
Incident Response and Resolution: Respond to and resolve production incidents, prioritizing guest-facing issues to minimize disruption. Conduct root cause analysis with guidance from senior team members and implement preventive measures to avoid recurrence.
Monitoring and Observability: Build, maintain, and enhance monitoring tools and dashboards (using Prometheus, Grafana, or similar) to provide visibility into system health, performance, and guest impact. Proactively detect and address potential issues.
Automation and Tooling: Develop and implement automation scripts and tools to streamline operations, reduce manual intervention, and improve system reliability. Utilize configuration management tools and infrastructure as code principles.
Collaboration: Work closely with product teams to incorporate reliability principles into new feature development. Collaborate with operations teams to ensure smooth deployments and transitions.
Documentation and Knowledge Sharing: Create and maintain clear documentation on system architecture, troubleshooting guides, and incident postmortems. Share knowledge and best practices with the team.
On-Call Support: Participate in on-call rotation as defined by team needs, primarily focusing on acknowledging and escalating incidents, with guidance from senior team members.
Working Hours: Expectations of non-standard working hours which include mornings, nights, and weekend rotations.
 
Knowledge and Skills: 
Technical Expertise: Strong knowledge of mobile (iOS, Android) and web technologies, backend systems, cloud infrastructure (AWS, Azure, etc.), and database technologies.
Programming: Proficiency in one or more programming languages (e.g., Python, Java, Go, Jenkins) for scripting and automation. Working knowledge of and Kubernetes a high plus.
Monitoring and Observability: Experience with tools like Prometheus, Grafana, Splunk, or similar.
Incident Management: Experience with incident management tools like PagerDuty, ServiceNow, or similar.
Security: Understanding of security best practices, vulnerability identification, and incident response.
Communication: Excellent written and verbal communication skills for collaborating with diverse teams and stakeholders.
Customer Service: Understands and is aligned to the purpose of providing a great client experience (client focused attitude)
Detailed Oriented: The ability to understand and appreciate the fine, granular details.
SQL Database: Ability to work with large volumes of customer data. Ability to use Oracle SQL (or similar) to query databases and perform edits to SQL queries.
Preferred Qualifications: 
5+ years of demonstrated proficiency in one or more scripting languages such as python, Go, etc
3+ years of experience with Kubernetes or equivalent
5+ years of Software development experience in Java, JavaScript etc.
3+ years of experience with containers and container orchestrators - Docker, Kubernetes
5+ years of demonstrated experience debugging and fixing system/infrastructure and application issues
5+ years of experience working with monitoring tools such as Prometheus, Grafana, Splunk, Google stack driver, etc.
5+ years of experience with databases (SQL or NoSQL)
5+ years of experience with log analysis and building dashboards
At least 6 years in a Reliability Engineering, DevOps or infrastructure focused role
Advanced experience with programming languages ( Python, Java)
Deep systems and infrastructure knowledge
Excellent troubleshooting and problem-solving skills
Experience with high-traffic, guest-facing systems.
Our benefits package includes:
Comprehensive medical benefits
Competitive pay
401(k) retirement plan
...and much more!
About INSPYR Solutions
Technology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients' business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.

INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities
 
 
Desired Skills and Experience
RELIABILITY ENGINEER
SITE RELIABILITY ENGINEER
SRE
SOFTWARE ENGINEER
DEV OPS ENGINEER
CLOUD
AWS
AZURE
DOCKER
KUBERNETES
PYTHON
JAVA

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4259859439,https://www.linkedin.com/jobs/view/4259859439,Pending,Unknown,2025-07-09 00:19:59.889096,Asking for Security clearance,"
About the job
The Infrastructure Engineer works on holistic engineering deliverables across different stages of the product lifecycle and determines technology patterns for the overall solution. 
  Experience with Kubernetes - OpenShift(OCP) at a platform level
Experience with any one OpenShift / K8s supported Networking CNI. (Plus if you know Calico ) 
Experience with AWS infrastructure (VPC / Security groups / EC2 / Storages / Load Balancers) 
Experience doing OCP upgrades and creating OCP Clusters
Experience of platform automation products like RH ACM / Terraform / Ansible
Responsible for incident resolution and Pager Support on a rotation basis.
Achieves product commitments (and influences others to do the same) by using informal leadership & highly developed communication skills and contributes to or led technology communities.
Uses automation, system tools, open-source solutions, observability and 'security first' principles in daily work 

Preferred Qualifications 
Kubernetes certification like - CKA or CKAD and AWS certification.
Application programming skills in Golang, Java, etc.
Understanding of k8s operator model. 
Scripting languages like shell / Helm modules or some programming experience. 
Diagraming skills such as sequence diagrams, flowcharts, etc.
Concepts of secrets/credential management for Applications and systems - such as use of HashiCorp Vault
Operators in Kubernetes/OCP

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4262882007,https://www.linkedin.com/jobs/view/4262882007,Pending,Unknown,2025-07-09 00:20:18.553171,Required experience is high,"
About the job
Position - Senior Site Reliability Engineer
Location â€“ Charlotte, NC onsite
Contract to hire (C2H)
6 months contract after that convert into fulltime.
 Job Description: 
We are seeking a Senior Site Reliability Engineer (SRE) with deep expertise in AWS networking, infrastructure automation, and production system reliability and ability to lead when neededd. This role demands a strong grasp of observability, operational excellence, and the ability to drive the adoption of DevOps/SRE best practices across engineering teams. You will be instrumental in shaping SLIs/SLOs, defining our DevOps maturity roadmap, and building robust, scalable infrastructure using Terraform, Lambda, Step Functions, and more.
Youâ€™ll be leading a team of SREs and collaborating closely with DevOps, Security, and Application teams to ensure reliable delivery and availability of services. Lead and mentor a team of SREs in developing scalable infrastructure and operational processes. Design and implement SLIs, SLOs, and Error Budgets across critical services and evangelize them across product teams. Architect and manage AWS networking environments including VPCs, Transit Gateways, Route 53, VPNs, NACLs, and Security Groups. Manage and monitor Palo Alto and FortiGate firewalls, and integrate them with cloud environments for hybrid network visibility. Define and evolve DevOps maturity models, guiding teams toward higher automation and reliability. Build and manage observability dashboards using Grafana, CloudWatch and Datadog to track application and infrastructure health. Implement and maintain Infrastructure as Code (IaC) using Terraform to automate cloud deployments across environments. Develop and maintain serverless applications using AWS Lambda and Step Functions to support platform automation and operations. Collaborate with developers to define GitLab CI/CD pipelines and streamline the build, test, and deployment lifecycle. Champion incident response, blameless postmortems, and continuous improvement initiatives. Write scripts in Python or Bash to automate tasks and integrate systems
 REQUIREMENTS 
7+ years in SRE, DevOps, or Systems Engineering roles with increasing responsibility.
Proven experience managing AWS production environments with a focus on networking.
In-depth knowledge of Palo Alto and/or FortiGate firewall management and troubleshooting.
Expertise in monitoring and observability tools, including Grafana and Datadog.
Hands-on experience with Terraform in managing cloud infrastructure at scale.
Experience building and deploying serverless architectures using Lambda and Step Functions.
Demonstrated understanding of SLI/SLO design, error budgets, and reliability metrics.
Strong understanding of CI/CD principles and tools like GitLab CI/CD.
Proficiency in scripting using Python or Bash.
Nice to have Skills:
AWS Certifications (e.g., Solutions Architect, Advanced Networking, DevOps Engineer)
Familiarity with DevOps/SRE maturity models and implementing organizational transformation.
Experience with compliance frameworks (SOC2, ISO 27001, etc.) as they pertain to infrastructure reliability.
Familiarity with container orchestration is a plus.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4259896332,https://www.linkedin.com/jobs/view/4259896332,Pending,Unknown,2025-07-09 00:20:34.782892,Asking for Security clearance,"
About the job
Job Title: DevOps Engineer
Locations: Annapolis Junction, MD
Job Type: Onsite
 Full-Time/Permanent job position
  JOB DESCRIPTION:

 Senior level candidate needed with strong Kubernetes, Terraform and AWS. Top Secret/SCI Clearance/FSP Required.
   Role Responsibilities:
Provide optimization and automation across multiple platforms and applications.
Drive consistency for deployment and build processes.
Utilize modern tools like Jenkins, Docker, Kubernetes, Amazon Web Services, Ansible, Terraform, Python, Linux, and much more.
You have worked with teams before on large and demonstrable projects (preferably built on with AWS and familiarity with scripting languages such as Bash, Ruby, Python).
Familiarity with containerization tools like Docker and Kubernetes.
You understand how to set up a CI/CD pipeline.
You understand Agile software development and DevOps practices and can work closely with your peers other mid-to large size teams.
Focus on understanding customers' needs and translating those needs from product specifications into functional, production ready code and infrastructure.
The following skills are required: AWS (S3, VPCs & Networking, EC2, ECS/EKS). Containerization (Docker, k8s, Registries). IaC (Terraform/Cloud Formation). CI/CD (Jenkins/ GitHub Actions).
You are self-driven, self-learner willing to share knowledge and participate actively in a team environment.
CWIP IAT Level I required.
Experience above is required with a bachelorâ€™s degree.
4 years of additional experience can be substituted for a bachelorâ€™s degree.
4 years less experience is ok for candidates with a graduate degree.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4264256225,https://www.linkedin.com/jobs/view/4264256225,Pending,Unknown,2025-07-09 00:20:43.295848,Required experience is high,"
About the job
DevOps Engineer with Terraform Experience. Hybrid Model. 
Long Term W2 Contract in Jersey City or Pennington, NJ

8+ years of DevOps experience
Strong experience in Ansible, Jenkins, Terraform
Experience taking manual tasks/processes and automated at the enterprise level.
Demonstrated expertise of scripting via common solutions such as Python and PowerShell.
Exposure to working in environment that is heavy on operations technology (automation, optimization, rapid development)
Demonstrated expert level of RHEL Linux OS and LDAP Directory administration.
Solid understanding and demonstrable expertise delivering Linux based automated solutions.
Solid understanding of infrastructure (hardware, network, storage).
Good understanding of agile methodology and associated toolset (Jira, Bitbucket, Jenkins, etc.)
Develop and maintain automated solutions leveraging Terraform, Jenkins, Ansible, Git
Managing and optimizing cloud infrastructure on platforms like Azure and AWS

Experience required 8 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4262169934,https://www.linkedin.com/jobs/view/4262169934,Pending,Unknown,2025-07-09 00:20:53.165507,Asking for Security clearance,"
About the job
Position: DevOps Engineer
Location: Remote â€“ Chicago area 
Job Type: 6-Month W2 Contract 
Compensation: $50-$54/hr 

Description: 
The Infrastructure Engineer works on holistic engineering deliverables across different stages of the product lifecycle and determines technology patterns for the overall solution. This role takes the lead in guiding more junior engineers and is a role model in fostering the adoption of new technologies. Actively manages and escalates risk and customer-impacting issues within the day-to-day role to management. 
 Qualifications: 
Experience with Kubernetes - OpenShift(OCP) at a platform level
Experience with any one OpenShift / K8s supported Networking CNI. (Plus if you know Calico ) 
Experience with AWS infrastructure (VPC / Security groups / EC2 / Storages / Load Balancers) 
Experience doing OCP upgrades and creating OCP Clusters
Experience of platform automation products like RH ACM / Terraform / Ansible
Responsible for incident resolution and Pager Support on a rotation basis.
Achieves product commitments (and influences others to do the same) by using informal leadership & highly developed communication skills and contributes to or led technology communities.
Uses automation, system tools, open-source solutions, observability and 'security first' principles in daily work 
Contributes to team agile ceremonies, leads demos and presentations, helps new engineers learn established norms 
Responsible for pager support once every quarter.
Initiates high level solution design approaches, and guides team to achieve desired key software delivery capabilities using automated, coded enterprise and observability 
Participates in internal speaking and advocacy events 
Supports research activities to adopt new technology solutions in ways of developing new capabilities 
Continues professional education and creates opportunities for core product teams to learn engineering best practices 
Coaches immediate chapter and actively fosters the adoption of new technologies

Preferred Qualifications 
Bonus Points If You Have: 
Kubernetes certification like - CKA or CKAD and AWS certification.
Application programming skills in Golang, Java, etc.
Understanding of k8s operator model. 
Scripting languages like shell / Helm modules or some programming experience. 
Diagraming skills such as sequence diagrams, flowcharts, etc.
Concepts of secrets/credential management for Applications and systems - such as use of HashiCorp Vault
Operators in Kubernetes/OCP

TECHNICAL SKILLS
Must Have
Ansible
AWS
calico
CICD
Firewalls
Kubernetes
Kubernetes Openshift
Scripting

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4262158969,https://www.linkedin.com/jobs/view/4262158969,Pending,Unknown,2025-07-09 00:21:04.652851,Required experience is high,"
About the job
Sr. DevOps Engineer (Azure, GitHub Action, Ansible)
Fully Remote: Must be in EST or CST time zone
6-month contract-to-hire

Optomi, in partnership with our premier client in the insurance industry, is seeking an experienced DevOps Engineer to join their team for a remote role! The DevOps Engineer will support multiple product and development teams in automating infrastructure, streamlining CI/CD pipelines, managing deployments, and ensuring production readiness. The ideal candidate will bring deep expertise in Azure, Terraform, Ansible, and GitHub Actions, along with strong scripting and automation skills. You will collaborate with product managers, architects, developers, and stakeholders in a fast-paced agile environment to deliver scalable and resilient cloud infrastructure solutions.

Key Responsibilities:
Design, build, and maintain cloud infrastructure using Azure services (Identity, Networking, Compute, Storage, Automation, Disaster Recovery).
Develop and maintain infrastructure-as-code using Terraform and Ansible.
Create, enhance, and support CI/CD pipelines using GitHub Actions and GitHub Enterprise.
Write scripts and automation tools using Python, Bash, and PowerShell.
Operate and scale container platforms; support containerization and migration efforts.
Manage high-availability, cloud-based server architectures and enforce deployment standards.
Implement monitoring and alerting strategies; define telemetry and observability baselines.
Partner with cross-functional teams to troubleshoot and resolve production issues.
Apply DevOps and Agile best practices for continuous integration and delivery.
Document infrastructure components, automation tools, and deployment procedures.
Provide mentoring and technical guidance to junior DevOps engineers.

Required Qualifications:
7+ years of IT experience, with at least 2+ years in DevOps automation/integration.
Proven experience with:
Microsoft Azure (IaaS, PaaS, Networking, Identity, and Resource Management)
Terraform (Infrastructure as Code)
Ansible (Configuration Management)
GitHub Actions and GitHub Enterprise
Scripting and automation expertise in Python, Bash, and PowerShell.
Strong Linux administration skills and understanding of system internals.
Familiarity with software development life cycles and release management.
Experience with monitoring tools and defining performance metrics and thresholds.
Strong understanding of networking concepts and protocols.
Comfortable working in multi-cloud or hybrid environments.
Excellent problem-solving, communication, and documentation skills.

Preferred Qualifications:
Bachelorâ€™s degree in Computer Science or related field (or equivalent experience).
Relevant certifications (e.g., Azure Administrator, Terraform Associate, RHCE).
Experience in regulated or enterprise environments (e.g., insurance or finance).
Prior experience in Agile environments and test-driven development methodologies.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4261824324,https://www.linkedin.com/jobs/view/4261824324,Previous resume,Unknown,2025-07-09 00:21:50.982831,Problem in Easy Applying,"Seems like stuck in a continuous loop of next, probably because of new questions.",Easy Applied,4261824324 - Failed at questions - 2025-07-09 00:21:47.692521.png
4262160401,https://www.linkedin.com/jobs/view/4262160401,Pending,Unknown,2025-07-09 00:22:13.941548,Required experience is high,"
About the job
Optomi, in partnership with a global leader in IT infrastructure services, is seeking a Lead DevOps Engineer to drive the future of automation and infrastructure in an Azure-native environment.

This is a high-impact, hybrid technical/strategic role where youâ€™ll lead intelligent automation initiatives, embed AI capabilities into CI/CD and infrastructure workflows, and guide a senior DevOps engineer. Youâ€™ll collaborate across architecture, platform, and software teams to modernize operations and scale SaaS-focused solutions using Azure Fabric and Microsoft-native tools.


Key Responsibilities
Architect and scale CI/CD pipelines using GitHub Actions and Azure DevOps tools, embedding AI/ML models for intelligent testing, deployment, and observability.
Lead implementation of predictive monitoring and alerting with Dynatrace and Azure Monitor to enable self-healing systems.
Define and execute infrastructure automation strategies with Terraform, aligning with governance and long-term scalability goals.
Mentor and guide DevOps engineers, fostering autonomy, technical excellence, and innovation.
Collaborate with architecture, data, and engineering teams to align automation strategy with enterprise-wide SaaS and Azure Fabric adoption.
Evaluate and adopt emerging Azure AI services (e.g., Azure Machine Learning, Cognitive Services) to enhance infrastructure intelligence.
Serve as liaison across teams, driving DevOps priorities, best practices, and automation-first culture.

Core Requirements
7+ years in DevOps or cloud engineering with hands-on technical leadership experience.
Deep expertise with GitHub Actions or similar CI/CD tooling in enterprise Azure environments.
Advanced experience with Terraform for large-scale infrastructure automation.
Proven knowledge of Dynatrace, Azure Monitor, or similar observability tools with AI/ML-based alerting strategies.
Demonstrated ability to mentor engineers, influence technical culture, and lead DevOps transformation.
Hands-on experience with Azure AI services and embedding intelligence into infrastructure or DevOps workflows.
Excellent communication skillsâ€”able to work cross-functionally and drive clarity from ambiguity.

Preferred Qualifications
Experience with Azure Fabric and transitioning to SaaS-native architectures.
Background in both infrastructure and software engineering with an SRE/DevOps mindset.
Familiarity with data platform automation and scaling distributed systems.
Ability to balance architectural governance with flexibility and innovation.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4260739147,https://www.linkedin.com/jobs/view/4260739147,Pending,Unknown,2025-07-09 00:22:28.154585,Asking for Security clearance,"
About the job
Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose â€“ the relentless pursuit of a world that works better for people â€“ we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI. 
 Inviting applications for the role of 
Core Platform Engineer â€“ Kubernetes, K3s, Docker, OpenShift.
The ideal candidate will be responsible for Core Platform Engineer to design, build, and maintain container orchestration and infrastructure platforms using Kubernetes, K3s, Docker, and OpenShift..
 Responsibilities
Â· Design and maintain production-grade Kubernetes clusters (on-prem and/or cloud).
Â· Deploy and manage lightweight K3s clusters for edge or resource-constrained environments.
Â· Implement and maintain Docker-based container runtimes, images, and registries.
Â· Manage OpenShift clusters, including setup, operator lifecycle management, security policies, and application deployment strategies.
Â· Develop and manage CI/CD pipelines integrated with Kubernetes/OpenShift (e.g., Tekton, ArgoCD, GitLab CI).
Â· Implement Infrastructure as Code (IaC) using tools like Terraform, Helm, and Ansible.
Â· Secure container environments through network policies, role-based access control (RBAC), secrets management, and vulnerability scanning.
Â· Work closely with application teams to onboard workloads, troubleshoot issues, and optimize resource usage.
Â· Participate in on-call rotations and provide L3 support for platform-related incidents.
Â· Develop and maintain Linux shell scripts for automation of deployments, monitoring, and maintenance tasks.
  Qualifications we seek in you!
Minimum Qualifications
Â· BE/B Tech/MCA
Â· Excellent written and verbal communication skills
Â· Min 7 years of experience.
Â· Experience in Finance Domain
   Preferred Qualifications/ Skills
Â· Strong hands-on experience with Kubernetes (EKS, AKS, GKE, or vanilla) cluster administration.
Â· Experience with K3s in edge computing, dev/test environments, or IoT scenarios.
Â· Proficiency in Docker image creation, optimization, and lifecycle management.
Â· Solid experience with Red Hat OpenShift (v4+), including Operators, SCCs, Routes, and Service Mesh.
Â· Strong scripting/automation skills in Bash, Python, or Go.
Â· Familiarity with container storage, networking, and ingress controllers (e.g., NGINX, Istio, Traefik).
Â· Deep understanding of CI/CD tools and GitOps practices.
Â· Solid grasp of Linux internals, system performance tuning, and security best practices. 
Â· Work-from-Anywhere Roles â€“ â€œLos Angeles California-based candidates are not eligible for this roleâ€ 
Â· Location-based Roles (e.g., Richardson roles â€“ metro area can be adjusted by role location) â€“ â€œLos Angeles, California based candidates are not eligible for this role. area candidates are eligible for this role only.â€
 Genpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com . Follow us on Twitter, Facebook, LinkedIn, and YouTube.
Furthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4260713690,https://www.linkedin.com/jobs/view/4260713690,Pending,Unknown,2025-07-09 00:22:30.582894,Required experience is high,"
About the job
Location: Hybrid Herndon, VA
Base Salary Range: $170,000 - $200,000 annually plus 25% annual bonus

Our client builds and operates LEO satellites for RF data collection and analysis. As a member of the Platform Engineering team, you will enable fellow team members to build secure, performant, efficient, and cost-effective systems and tools supporting the company's mission. The successful candidate will help shape the next generation platform by providing expert insights to improve the existing system and turn ideas into deployed production capabilities. The Principal Platform Engineer will report to the Director of Platform Engineering, and interface with directors and engineers throughout the Engineering organization.

Responsibilities -
Develop, deploy, and maintain critical infrastructure in AWS for the company's collection, processing, and analytics services.
Provide expert technical guidance and oversight across the entire company's platform architecture.
Collaborate across engineering teams to understand platform requirements, use cases, and functionality.
Design and improve platform architecture to meet the functional needs of the organization and ensure integrity and security of the systems and data.
Troubleshoot and perform root cause analysis on unexpected issues/outages.
Guide and mentor teammates to automate and simplify daily development and deployment processes.

Qualifications -
M.S. or B.S. in Computer Science or related engineering field or equivalent experience
10+ years professional experience
Broad knowledge of Infrastructure setup, automation and troubleshooting, with specific knowledge of the Amazon Web Services (AWS) technology stack; particularly deploying and maintaining EKS, EC2, RDS, S3/EBS/EFS.
A deep understanding of basic networking concepts relevant to Platform Engineering troubleshooting, including TCP/IP, UDP, HTTP, firewalls, basic routing concepts, and load balancers, with hands-on AWS networking experience
Knowledge of Linux operating systems and installing and configuring security patches, within containers and on servers/virtual machines
Deep understanding of development and security best-practices, utilizing least-privilege concepts for access control
Experience with Software/Systems Operations, to include managing and troubleshooting production systems
Experience deploying and configuring applications to run on Kubernetes clusters using Helm
Familiarity with writing and maintaining terraform modules to deploy infrastructure
Excellent written and oral communication skills

Preferred Skills -
Experience with Kubernetes advanced add-ons such as: ArgoCD, Kyverno, Cilium, and Crossplane
Experience migrating terraform modules to Crossplane compositions
Experience with Keycloak, Okta or other OIDC/SAML-based SSO & Auth services
Experience building and updating secure Docker images to deploy custom applications
Familiarity Metrics & Monitoring tooling (Grafana, Mimir, Loki, Tempo, or similar)
Experience with Tableau and Snowflake

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4264258655,https://www.linkedin.com/jobs/view/4264258655,Pending,Unknown,2025-07-09 00:22:32.954273,Required experience is high,"
About the job
Role : Azure DevOps Engineer
Location : California and Nevda ( Onsite) 
contract role 
Experience required : 13 years 

10+ years of experience in Cloud Engineering, DevOps, or related roles.
Strong expertise in Terraform IAC for Infrastructure as Code (IaC).
Proficiency in CI/CD tools like Jenkins, GitHub Actions, or GitLab CI/CD.
Knowledge of containerization technologies (Docker, Kubernetes).

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4260751210,https://www.linkedin.com/jobs/view/4260751210,Pending,Unknown,2025-07-09 09:46:58.549597,Required experience is high,"
About the job
Our client is currently seeking a Senior DevOps Engineer 

Overview
In this role, you will be responsible for the design, development, and deployment of scalable and secure software solutions across the full software development lifecycle. You will work closely with both front-end and back-end technologies, with a strong focus on Java, Spring Boot, Azure (AKS), and DevOps practices.
You will contribute to building robust microservices, integrating cloud-native solutions, and ensuring continuous integration and delivery pipelines are efficient and secure. This role follows Agile development methodologies and requires strong collaboration and problem-solving skills.
Minimum Qualifications
8â€“10 years of experience in software development and DevOps engineering.
Proficiency in Java (1.8, 21) and Spring Boot frameworks.
Experience with cloud platforms, especially Azure and Azure Kubernetes Service (AKS).
Strong understanding of DevOps principles, CI/CD pipelines, and infrastructure automation.
Hands-on experience with Docker, GitHub, and Gradle/Maven.
Familiarity with Unix/Linux environments and shell scripting.
Experience with SQL and NoSQL databases (e.g., Oracle, Cosmos DB, MarkLogic).
Knowledge of microservices architecture and cloud-native design patterns.
Preferred Qualifications
Experience with Azure DataBricks, Azure Data Factory (ADF).
Familiarity with Kafka, Python, and JUnit for testing.
Exposure to Spring Integration and Spring Batch.
Experience with PCF (Pivotal Cloud Foundry) or similar cloud platforms.
Proficiency with development tools such as IntelliJ or Eclipse.
Rate: $70-80/HR

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4263343963,https://www.linkedin.com/jobs/view/4263343963,Pending,Unknown,2025-07-09 09:47:22.849368,Asking for Security clearance,"
About the job
Title: Platform Engineer 
Location: Toronto, (Onsite)
Experience: 8 â€“ 12 Years 

Job Description: 
âˆ™Architect, develop, and maintain cloud infrastructure solutions across AWS, Azure, and GCP. 
âˆ™Implement automated deployments using Kubernetes (EKS), ArgoCD, Helm, and Terraform. 
âˆ™Develop and maintain GitLab CI/CD pipelines to streamline and secure software delivery. 
âˆ™Script automation tools and solutions using Python and Golang to optimize operational efficiency. 
âˆ™Integrate secure secret management practices using HashiCorp Vault. 
âˆ™Collaborate closely with cross-functional teams to understand and meet project requirements. 
âˆ™Proactively manage tasks and deadlines, adapting swiftly to project shifts and agile demands. 
âˆ™Extensive experience in cloud architecture including AWS, Azure, and GCP. 
âˆ™Strong expertise in Kubernetes automation and management via Helm charts and ArgoCD. 
âˆ™Proficient in infrastructure as code using Terraform. 
âˆ™Advanced knowledge of Python and Golang for automation tasks.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4264126298,https://www.linkedin.com/jobs/view/4264126298,Pending,Unknown,2025-07-09 09:47:32.946683,Found a Bad Word in About Job,"
About the job
ðŸŒŸ Principal DevOps Engineer with AI 

Full-Time | Hybrid (3 Days Onsite â€“ New Jersey) | No Visa Sponsorship

Join a Cutting-Edge AI & DevOps Innovation Team
We're seeking a Principal DevOps Engineering Specialist with AI to play a pivotal role in advancing Large Language Model (LLM) capabilities within a modern DevOps ecosystem. This hybrid, full-time position (3 days onsite in New Jersey) is ideal for a hands-on, highly skilled engineer passionate about GenAI, automation, and AI-driven development practices.

ðŸ” What You'll Do
As a core member of the engineering team, you'll:
Drive enhancements to our DevOps processes including automation of requirements gathering, code generation, and test creation.
Design, implement, and refine LLM-based prompts to guide GenAI outputs that are accurate, relevant, and secure.
Contribute to the development of internal tools and frameworks that support software delivery and improve team productivity.
Work collaboratively across cross-functional development teams and third-party vendors to ensure smooth integration and scalable systems.
Stay ahead of the curve by exploring new GenAI techniques, tools, and models to drive innovation.
Lead initiatives in documentation, automation testing, and process reliability.

ðŸ’¡ Tech Youâ€™ll Work With
LLMs (Multiple models)
Microsoft Azure & Azure DevOps Server
GitHub Copilot
Bash, Ansible (Automation tools)
CI/CD, Infrastructure as Code, DevOps frameworks
Service-Oriented Architectures (SOA)

âœ… Your Background
Proven experience improving DevOps pipelines and implementing automation strategies.
Hands-on experience designing prompts and workflows using Generative AI / LLMs.
Knowledge of cloud platforms, especially Azure.
Familiarity with modern software development methodologies and scripting tools.
Strong collaboration and documentation skills.
At least 4â€“6 years of relevant experience and a BA/BS in Computer Science or related field.
2+ years working with LLMs in a development or automation context.

ðŸš« Please Note
No Visa Sponsorship Available â€“ You must be authorized to work in the U.S.
No C2C / No Third-Party Vendors

ðŸ¢ About Hirobe Limited
Expert recruitment, powered by a deep understanding of the Adobe Experience Cloud.
Since 2018, we've been connecting the best Adobe professionals with leading organizations, giving us a unique insight into both the technology and the talent. What truly sets us apart is our specialized market knowledge, tailored service, and unwavering commitment to finding the ideal fit for everyone involved.

ðŸ“¬ Ready to Apply?
This is an urgent hire!
ðŸ“§ Send your resume to: raj@hirobe.io
ðŸ“ž Or call: +1 917-512-8959

Letâ€™s redefine whatâ€™s possible with AI and automationâ€”together.

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4259553670,https://www.linkedin.com/jobs/view/4259553670,Pending,Unknown,2025-07-09 09:47:47.802045,Required experience is high,"
About the job
Title: Senior DevOps Engineer
Location: Englewood, CO
Duration: 9-month contract, potential extensions
Schedule: Hybrid- 4 days onsite/week
Pay Rate: 65-75/hr
 Required Skills & Experience
10+ years of DevOps Experience
Python and/or Ansible
Kubernetes
Docker
Jenkins
 Nice to Have Skills & Experience
Background in telecom/cable
VMware
Grafana
MLOps
SRE/Infrastructure 
 Job Description
As a DevOps Engineer IV in Platform as a Service for on-premises cloud, you will be responsible for ensuring the reliability, availability, and scalability of our PaaS infrastructure, IaaS platforms, cloud management platform, CI/CD pipeline, automation, and tooling. You will work closely with our development, operations, and security teams to design, implement, and maintain a highly available and secure PaaS platform and managed services systems. Your primary focus will be on design, developing, and maintaining the service catalog, CI/CD pipelines, and automation required for our PaaS offerings, managed services tools, ensuring high availability, reliability, and performance of our services. You will be responsible for ensuring that our PaaS platforms, cloud management platform, and other managed services meets the needs of our internal and external stakeholders.
Design, develop, implement, and maintain Grafana Data Reporting and Visualization with specific familiarity working with VMWare VROPS data collection and presentation.
Automate deployment, monitoring, and management of PaaS and IaaS services.
Works with developers, testers, and deployment teams to create software deployment plans.

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4257702677,https://www.linkedin.com/jobs/view/4257702677,Pending,Unknown,2025-07-09 09:47:57.981471,Required experience is high,"
About the job
We are looking for a skilled DevOps Engineer to join our team in Madison, Wisconsin. This position offers an exciting opportunity to work on diverse projects and utilize cutting-edge technologies. The ideal candidate will bring strong expertise in automation, infrastructure management, and continuous integration pipelines, along with a passion for optimizing system performance and reliability.

Responsibilities:
â€¢ Develop, implement, and maintain CI/CD pipelines using tools such as GitLab CI and Terraform.
â€¢ Manage infrastructure automation using Ansible and other configuration management tools.
â€¢ Monitor system performance and ensure observability using technologies like Prometheus and Loki.
â€¢ Build and maintain virtual machine images with tools like Packer.
â€¢ Collaborate with cross-functional teams to enhance system reliability and scalability.
â€¢ Analyze and mitigate risks by maintaining comprehensive risk registers.
â€¢ Provide leadership and technical guidance to support operations and project goals.
â€¢ Design and implement strategies for site reliability to optimize application performance.
â€¢ Utilize AWS technologies, including Amazon EC2, for infrastructure deployment and management.
â€¢ Drive innovation and efficiency in DevOps practices by leveraging modern tools and methodologies.
â€¢ Minimum of 7 years of experience in DevOps engineering or related roles.
â€¢ Proficiency in automation tools such as Ansible and Terraform.
â€¢ Hands-on experience with CI/CD platforms, including GitLab CI.
â€¢ Strong knowledge of observability tools like Prometheus and Loki.
â€¢ Familiarity with AWS technologies, including Amazon EC2 and related services.
â€¢ Ability to manage and support infrastructure using configuration management tools.
â€¢ Solid understanding of site reliability engineering principles.
â€¢ Experience with Agile methodologies and collaborative development environments.
Technology Doesn't Change the World, People Do.Â®

Robert Half is the worldâ€™s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.


Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go. Download the Robert Half app and get 1-tap apply, notifications of AI-matched jobs, and much more.


All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit roberthalf.gobenefits.net for more information.


Â© 2025 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking â€œApply Now,â€ youâ€™re agreeing to Robert Halfâ€™s Terms of Use.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4261141632,https://www.linkedin.com/jobs/view/4261141632,Pending,Unknown,2025-07-09 09:48:02.311238,Required experience is high,"
About the job
DataStaff, Inc is seeking a Cloud Engineer for a long-term contract opportunity with one of our direct clients located in Raleigh, NC

* This position is hybrid

Job Description:
Our client is seeking an AWS Engineer specializing in Data modernization using cloud solutions on AWS. This role will work closely with data modernization team to design, implement, and manage AWS solutions that meet their technical requirements and business objectives.

Responsibilities:
Collaborating with stakeholders to gather requirements and propose effective modernization strategies
Providing technical guidance and troubleshooting support throughout project delivery
Acting as a trusted advisor to the technical team on industry trends and emerging technologies
Develop, test, debug and deliver within timeline using best tools and best practices available.
Ability to identify, analyze, and resolve technical issues and challenges.
Designing and implementing complex, scalable, and secure AWS solutions tailored to customer needs
Identifies architectural risks and plans to mitigate risks
Ensures adherence to standards and best practices
Influences and communicates effectively with non-technical audiences including senior ITD and business stakeholders.

Required Skills:
5 Years - Proficient in using various AWS services (e.g., VPC, EC2, S3, Lambda, ECS, EKS, RDS, API Gateway, Glue Crawlers, Athena) to build and manage cloud infrastructure.
5 Years - Experience in modernizing applications, including refactoring, migration, and cloud-native development.
5 Years - Knowledge of modern database technologies and strategies for migrating and modernizing database systems
5 Years - Experience with DevOps practices and automation tools
7 Years - Strong communication and collaboration skills to work with cross-functional teams, including developers, business analysts, and stakeholders.
3 Years - Experience working with creating AWS ETL jobs, data pipelines with high volumes of data

Desired Skills:
Experience with data analytics, business intelligence solutions (like Power BI or AWS QuickSight)
AWS Certification
State Government experience

This opportunity is available on a corp-to-corp basis or as a W2 position with a competitive benefits package. DataStaff, Inc. offers medical, dental, and vision coverage options as well as paid vacation, sick, and holiday leave. As many of our opportunities are long-term, we also have a 401k program available for employees after 6 months.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4258078542,https://www.linkedin.com/jobs/view/4258078542,Pending,Unknown,2025-07-09 09:48:19.448161,Problem in Easy Applying,"Message: 
Stacktrace:
0   chromedriver                        0x0000000104e34e6c cxxbridge1$str$ptr + 2722840
1   chromedriver                        0x0000000104e2cd74 cxxbridge1$str$ptr + 2689824
2   chromedriver                        0x000000010497e3ec cxxbridge1$string$len + 90648
3   chromedriver                        0x00000001049c5544 cxxbridge1$string$len + 381808
4   chromedriver                        0x0000000104a06934 cxxbridge1$string$len + 649056
5   chromedriver                        0x00000001049b9834 cxxbridge1$string$len + 333408
6   chromedriver                        0x0000000104df7f88 cxxbridge1$str$ptr + 2473268
7   chromedriver                        0x0000000104dfb1f4 cxxbridge1$str$ptr + 2486176
8   chromedriver                        0x0000000104dd99d0 cxxbridge1$str$ptr + 2348924
9   chromedriver                        0x0000000104dfbab0 cxxbridge1$str$ptr + 2488412
10  chromedriver                        0x0000000104dcaa60 cxxbridge1$str$ptr + 2287628
11  chromedriver                        0x0000000104e1b9a0 cxxbridge1$str$ptr + 2619212
12  chromedriver                        0x0000000104e1bb2c cxxbridge1$str$ptr + 2619608
13  chromedriver                        0x0000000104e2c9b0 cxxbridge1$str$ptr + 2688860
14  libsystem_pthread.dylib             0x00000001947bac0c _pthread_start + 136
15  libsystem_pthread.dylib             0x00000001947b5b80 thread_start + 8
",Easy Applied,Not Available
4264749156,https://www.linkedin.com/jobs/view/4264749156,Pending,Unknown,2025-07-09 09:48:31.725381,Found Blacklisted words in About Company,Failed to find an element with given classes,Skipped,Not Available
4264743524,https://www.linkedin.com/jobs/view/4264743524,Pending,Unknown,2025-07-09 09:48:48.728531,Found Blacklisted words in About Company,Failed to find an element with given classes,Skipped,Not Available
4264749156,https://www.linkedin.com/jobs/view/4264749156,Pending,Unknown,2025-07-09 09:49:00.186284,Found Blacklisted words in About Company,Failed to find an element with given classes,Skipped,Not Available
4262052885,https://www.linkedin.com/jobs/view/4262052885,Pending,Unknown,2025-07-09 09:49:11.389165,Found Blacklisted words in About Company,Failed to find an element with given classes,Skipped,Not Available
4260611037,https://www.linkedin.com/jobs/view/4260611037,Pending,Unknown,2025-07-09 09:49:22.250862,Found Blacklisted words in About Company,Failed to find an element with given classes,Skipped,Not Available
4264749156,https://www.linkedin.com/jobs/view/4264749156,Pending,Unknown,2025-07-09 09:49:32.593752,Found Blacklisted words in About Company,Failed to find an element with given classes,Skipped,Not Available
4259410810,https://www.linkedin.com/jobs/view/4259410810,Pending,Unknown,2025-07-09 09:49:43.565194,Found Blacklisted words in About Company,Failed to find an element with given classes,Skipped,Not Available
4264258655,https://www.linkedin.com/jobs/view/4264258655,Pending,Unknown,2025-07-09 09:49:53.999264,Found Blacklisted words in About Company,Failed to find an element with given classes,Skipped,Not Available
4253769303,https://www.linkedin.com/jobs/view/4253769303,Pending,Unknown,2025-07-09 09:50:07.960131,Required experience is high,"
About the job
AWS bedrock Engineer
Coppell, TX Onsite
Full Time

Experience : 8+ Years
Must Have Technical/Functional Skills
â€¢ Knowledge and experience in AWS Bedrock
â€¢ Knowledge of Python
â€¢ Knowledge of AWS
â€¢ Experience in working with Architects
â€¢ Understanding of Foundation Models and LLMs
â€¢ Ability to customizing models with own data, requiring knowledge of techniques like fine-tuning and retrieval-augmented generation (RAG). 
â€¢ Ability to integrates with other AWS services, such as Lambda and OpenSearch

Roles & Responsibilities
â€¢ Design/develop foundation models for specific business use cases using AWS Bedrock tools. â€“ 
â€¢ Integrate generative AI features into enterprise platforms and workflows. 
â€¢ Collaborate with various stakeholders like data scientists, ML engineers, and developers to build scalable, AI-powered solutions.

Shweta Sharma
shweta.sharma@msrtechnologies.com

Experience required 8 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4260265884,https://www.linkedin.com/jobs/view/4260265884,Pending,Unknown,2025-07-09 09:50:10.627587,Required experience is high,"
About the job
Job Title: Azure Cloud Engineer - W2 Only - We can provide sponsorship 
Duration: Long Term 
Location: Westlake, TX/Boston, MA - Hybrid 

Required Skills: 
9+ Years of IT Experience 
Azure 4+ years in recent projects (NOT AWS) 
Windows related cloud experience 
Set up windows machine 
Deploy to windows server 
Network set up Rehydration 
Cloud infrastructure

Responsibilities
Managing Azure VMs, storage, networking, Azure AD (Entra ID), Azure Backup, and ensuring patching compliance. Responding to alerts and performance issues using Azure monitoring tools and implementing security best practices
Maintaining and troubleshooting Azure-hosted servers, managing Active Directory, resolving server issues, performing upgrades and patch management, and ensuring security software is up to date
Automating operations using Jenkins Core, Azure CLI, ARM templates, implementing proactive monitoring, and optimizing application performance and scaling resources in Azure
Building CICD automation for Azure Windows VM deployment and rehydration
Design, develop and maintain comprehensive and scalable DevOps automation practice for Java applications,Python, .Net, NodeJs on Azure cloud platforms.
Design and develop the automation for Windows VM, AKS, Database, Service Bus, FunctionApp , Logic App, VMSS, AppGateway,VM , Storage, KeyVault solutions.
Create and implement cloud observability solutions, including dashboards and alerts, for various Azure services.
Establish system-level performance alerts and automate elasticity based on specified requirements.
Design and develop cost optimization solution for azure infrastructure
Maintain the detailed documentation, diagrams, and knowledge base.
Demonstrate commendable knowledge of the Azure tech stack.
Design and develop infrastructure cost dashboard to monitor and optimize expenses.
Develop automation for AWS S3 and EC2
Work with other developers, Squad lead and Chapter Leader of project delivery and skill set development
Hands-on experience with the API and UI Design & Development and Micro Services Architecture

Experience required 9 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4256316435,https://www.linkedin.com/jobs/view/4256316435,Pending,Unknown,2025-07-09 09:50:15.064761,Asking for Security clearance,"
About the job
Millennium Corporation is hiring an Infrastructure Engineer to work in our Huntsville, AL office. The candidate must be eligible to obtain a security clearance. 
 The Infrastructure Engineer develops and maintains automated infrastructure solutions to support k>fivefour's cybersecurity training operations. The Infrastructure Engineer specializes in infrastructure as code implementation using Ansible and Terraform, web application integration through API integration, and the deployment of scalable training environments.
  Design and implement infrastructure automation solutions using Ansible and Terraform to support consistent and repeatable deployment of training environments and cybersecurity laboratories.
Install and configure enterprise level virtual training environment in a high availability Proxmox cluster.
Configure, deploy, and maintain Ceph distributed storage clusters to provide scalable, resilient software defined storage solutions.
Develop and maintain webhook integrations between automation systems and web applications to enable seamless service delivery and dynamic environment provisioning.
Build and deploy infrastructure as code solutions that support the delivery of hands-on cybersecurity training scenarios and laboratory exercises within the Battlegrounds environment.
Collaborate with cyber training instructors and subject matter experts to translate training requirements into technical infrastructure specifications and automated deployment processes.
Monitor and maintain the health and performance of training infrastructure systems, implementing proactive maintenance schedules and troubleshooting procedures to ensure optimal availability.
Create and maintain comprehensive documentation for infrastructure automation processes, deployment procedures, and system configurations to support operational continuity.
Implement security best practices and compliance standards within infrastructure automation workflows to maintain the integrity of training environments and protect sensitive data.
Support the testing and validation of new infrastructure deployments and automation processes to ensure reliability and consistency across training delivery platforms.
Participate in capacity planning activities to ensure infrastructure systems can accommodate current and future training program requirements.
Provide technical support for infrastructure-related issues that impact training operations and work collaboratively with team members to resolve system problems efficiently.
Additional duties as assigned.
Qualifications
Expert-level proficiency in Ansible with demonstrated ability to design sophisticated automation workflows for enterprise systems and large-scale deployments
Extensive experience with Proxmox or other hypervisor technologies
Working knowledge of Ceph or other software defined storage solutions
Extensive experience with git version control systems
Professional certifications (Certified Ansible Specialist, Certified Terraform Associate) or relevant training (Proxmox training, Ceph training, Ansible training)
Excellent oral and written communication skills
Experience in working in high performing teams
Strong analytical and problem-solving skills, and the ability to navigate complex compliance issues
Business Development
Assist with Business Development activities as required to support Millennium's strategic business objectives, which may include but not limited to participation in technical interviews, creation of technical documentation, general proposal writing support and proposal color reviews.
Physical Requirements
Must be comfortable with prolonged periods of sitting at a desk and working on a computer.
Must be able to lift up to 10-15 pounds at a time.
Travel Requirements
10% or less

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4264235941,https://www.linkedin.com/jobs/view/4264235941,seetha.pdf,Unknown,2025-07-09 19:29:23.313290,Problem in Easy Applying,"Message: stale element reference: stale element not found in the current frame
  (Session info: chrome=138.0.7204.94); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception
Stacktrace:
0   chromedriver                        0x0000000105410e6c cxxbridge1$str$ptr + 2722840
1   chromedriver                        0x0000000105408d74 cxxbridge1$str$ptr + 2689824
2   chromedriver                        0x0000000104f5a3ec cxxbridge1$string$len + 90648
3   chromedriver                        0x0000000104f600a0 cxxbridge1$string$len + 114380
4   chromedriver                        0x0000000104f623e4 cxxbridge1$string$len + 123408
5   chromedriver                        0x0000000104f6248c cxxbridge1$string$len + 123576
6   chromedriver                        0x0000000104fa2ec0 cxxbridge1$string$len + 388332
7   chromedriver                        0x0000000104fa2324 cxxbridge1$string$len + 385360
8   chromedriver                        0x0000000104f976ec cxxbridge1$string$len + 341272
9   chromedriver                        0x0000000104f96de4 cxxbridge1$string$len + 338960
10  chromedriver                        0x0000000104fe2934 cxxbridge1$string$len + 649056
11  chromedriver                        0x0000000104f95834 cxxbridge1$string$len + 333408
12  chromedriver                        0x00000001053d3f88 cxxbridge1$str$ptr + 2473268
13  chromedriver                        0x00000001053d71f4 cxxbridge1$str$ptr + 2486176
14  chromedriver                        0x00000001053b59d0 cxxbridge1$str$ptr + 2348924
15  chromedriver                        0x00000001053d7ab0 cxxbridge1$str$ptr + 2488412
16  chromedriver                        0x00000001053a6a60 cxxbridge1$str$ptr + 2287628
17  chromedriver                        0x00000001053f79a0 cxxbridge1$str$ptr + 2619212
18  chromedriver                        0x00000001053f7b2c cxxbridge1$str$ptr + 2619608
19  chromedriver                        0x00000001054089b0 cxxbridge1$str$ptr + 2688860
20  libsystem_pthread.dylib             0x00000001947bac0c _pthread_start + 136
21  libsystem_pthread.dylib             0x00000001947b5b80 thread_start + 8
",Easy Applied,Not Available
4261148753,https://www.linkedin.com/jobs/view/4261148753,Pending,Unknown,2025-07-09 19:29:27.915420,Asking for Security clearance,"
About the job
Akkodis is seeking an Azure Cloud Engineer for a full-time position with a client located in Portland, ME. Ideally, looking for a local candidate and not someone willing to relocate.

Salary Range: $90,000 to $100,000/ Year(The salary may be negotiable based on experience, education, geographic location, and other factors)

Title: Azure Cloud Engineer
Location: Portland, ME (Hybrid once in a quarter)

Job Description:
We are seeking a highly skilled and motivated Azure Cloud Engineer with a strong development background to join our dynamic engineering team. The ideal candidate will have a proven track record of designing, implementing, and managing scalable cloud infrastructure solutions on the Azure platform, leveraging Infrastructure as Code (IaC) principles and cutting-edge technologies. The Azure Cloud Engineer will play a pivotal role in driving our cloud initiatives, ensuring the reliability, security, and efficiency of our Azure cloud environments.

How you'll make an impact
Cloud Infrastructure Design and Implementation: Design and implement robust, scalable, and secure cloud infrastructure solutions on the Azure platform, adhering to industry best practices and company standards.
Infrastructure as Code (IaC): Utilize Terraform to define and manage Azure cloud infrastructure as code, ensuring consistency, repeatability, and traceability across environments.
Version Control: Manage and maintain version control systems, particularly using Git (github), to ensure codebase integrity and collaboration.
Cloud Automation: Develop and maintain automation scripts and tools using Python to streamline Azure cloud operations, deployments, and resource management tasks.


Experience you'll bring 
A minimum of 7+ years of hands-on experience in designing, implementing, and managing cloud infrastructure on the Azure platform.
Proven expertise in using Terraform for Infrastructure as Code (IaC).
Strong proficiency in Python scripting for automation and Azure cloud operations tasks.
Solid understanding of Windows operating systems for cloud deployments.

Certifications:
Possess the Microsoft Certified: Azure Administrator Associate certification or higher.

Skills:
Deep understanding of Azure cloud architecture principles, networking concepts, and security best practices.
Excellent problem-solving and troubleshooting skills.
Strong communication and collaboration skills, with the ability to work effectively in a team environment.
Self-motivated and proactive, with a passion for learning and staying up-to-date with the latest Azure cloud technologies.

Additional Preferred Qualifications:
Understanding of advanced networking concepts and experience with Azure networking services such as Vnet, VPN, and Load Balancing.
Understanding of PKI infrastructure (certificates, encryption keys, and authentication protocols).
Experience with containerization technologies (e.g., Docker, Kubernetes).
Experience with CI/CD tools such as Jenkins, GitLab, Github Actions, or Azure DevOps.
Experience with logging and monitoring products such as Splunk and Datadog.
Experience with Azure cloud cost optimization strategies.

Benefits include but are not limited to:
Medical/Dental/Vision
401K
PTO/Paid Holidays
To read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy.

The Company will consider qualified applicants with arrest and conviction records in accordance with federal, state, and local laws and/or security clearance requirements, including, as applicable:
Â· The California Fair Chance Act
Â· Los Angeles City Fair Chance Ordinance
Â· Los Angeles County Fair Chance Ordinance for Employers
Â· San Francisco Fair Chance Ordinance

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4260751210,https://www.linkedin.com/jobs/view/4260751210,Pending,Unknown,2025-07-09 19:29:32.283755,Required experience is high,"
About the job
Our client is currently seeking a Senior DevOps Engineer 

Overview
In this role, you will be responsible for the design, development, and deployment of scalable and secure software solutions across the full software development lifecycle. You will work closely with both front-end and back-end technologies, with a strong focus on Java, Spring Boot, Azure (AKS), and DevOps practices.
You will contribute to building robust microservices, integrating cloud-native solutions, and ensuring continuous integration and delivery pipelines are efficient and secure. This role follows Agile development methodologies and requires strong collaboration and problem-solving skills.
Minimum Qualifications
8â€“10 years of experience in software development and DevOps engineering.
Proficiency in Java (1.8, 21) and Spring Boot frameworks.
Experience with cloud platforms, especially Azure and Azure Kubernetes Service (AKS).
Strong understanding of DevOps principles, CI/CD pipelines, and infrastructure automation.
Hands-on experience with Docker, GitHub, and Gradle/Maven.
Familiarity with Unix/Linux environments and shell scripting.
Experience with SQL and NoSQL databases (e.g., Oracle, Cosmos DB, MarkLogic).
Knowledge of microservices architecture and cloud-native design patterns.
Preferred Qualifications
Experience with Azure DataBricks, Azure Data Factory (ADF).
Familiarity with Kafka, Python, and JUnit for testing.
Exposure to Spring Integration and Spring Batch.
Experience with PCF (Pivotal Cloud Foundry) or similar cloud platforms.
Proficiency with development tools such as IntelliJ or Eclipse.
Rate: $70-80/HR

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4262876312,https://www.linkedin.com/jobs/view/4262876312,Pending,Unknown,2025-07-09 19:29:36.719845,Asking for Security clearance,"
About the job
AI Driven Remote Care Platform for Seniors & Polychronic Patients

Staff DevOps Engineer 
Weâ€™re seeking an experienced and driven DevOps engineer to join our world-class engineering team at Cairns Health. This critical role will support our efforts at the intersection of conversational AI, radar technology, and cloud-native infrastructure. You will play a key role in scaling, securing, and automating the infrastructure that powers our LLM-backed digital health companion and embedded sensor platform. Our ideal candidate is a hands-on systems thinker with a bias for automation, strong ownership mindset, and a deep focus on reliability.
Duties & Responsibilities
Lead the design, implementation, and operation of cloud infrastructure in support of highly available, scalable, and secure services.
Own and manage our CI/CD pipelines to ensure seamless, automated deployments across environments.
Monitor production systems to maintain uptime, performance, and cost efficiency using modern observability tools.
Collaborate with software and hardware engineers to build resilient infrastructure that supports radar-based sensing, LLM workloads, and real-time communication.
Design and enforce infrastructure-as-code practices using Terraform, Helm, or similar tools.
Manage Kubernetes clusters on AWS (EKS), including deployments, upgrades, scaling, and debugging.
Build automated alerting and remediation workflows for common system failures.
Ensure system security and compliance via automated audits, access control, and secrets management.
Qualifications
Bachelorâ€™s degree in Computer Science, Engineering, or related field, or equivalent practical experience, with 7+ years in DevOps, Site Reliability Engineering (SRE), or infrastructure engineering.
Expert in cloud platforms, particularly AWS, including experience with services like EC2, S3, RDS, CloudWatch, EKS, ECS, IAM, and VPCs.
Strong experience with Docker and Kubernetes for container orchestration and workload management.
Proficient in infrastructure-as-code (IaC) tools such as Terraform, Pulumi, or AWS CDK.
Experience with CI/CD tools like GitHub Actions, CircleCI, or Jenkins and release automation.
Familiar with monitoring and observability tools such as Prometheus, Grafana, ELK/EFK stacks, or Datadog.
Skilled in scripting and automation using Python, Bash, or Go.
Deep understanding of Linux systems administration, networking, and security best practices.
Experience with log aggregation, distributed tracing, and real-time alerting systems.
Familiar with Agile/Scrum workflows, Git version control, and collaborative DevOps practices.
Excellent problem-solving and debugging skills; ability to diagnose issues across services and systems.
Strong communication skills and a collaborative, team-first attitude.
Benefits
We offer competitive compensation, equity & benefitsâ€”including medical, dental, vision, paid vacation/sick days, and 401(k) plans.
Locations
Strong preference is Sunnyvale, CA. (Otherwise Los Angeles, CA.)
About Cairns Health
Cairns Health (https://www.cairns.ai/) is creating a fundamentally better healthcare experience for people with chronic health conditions and those who care for them. We make healthcare more accessible by simplifying complex care plans, connecting care teams, and meeting patients where they live. Through our conversational AI, patients use their voice to interact with our digital care companion, who proactively gives medication reminders, symptom checks, behavioral nudges, and even engages in friendly conversation to ease loneliness. Cairns uses a device that includes radar to put the patient in context and passively monitors their activitiesâ€”including heart rate, breathing rate, and sleep stagesâ€”all without a wearable. The result is informed and timely intervention that drives improved clinical outcomes, reduced care delivery costs, and a more satisfactory healthcare experience for all.
To Apply
Please apply here. Then send your resume and cover letter to: nicky@cairns.ai.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4263373902,https://www.linkedin.com/jobs/view/4263373902,Pending,Unknown,2025-07-09 19:29:41.163139,Found a Bad Word in About Job,"
About the job
Position: DevOps Engineer
Clearance: Uncleared but must be US Citizen in order to receive a clearance - can hold up to TS clearance if active.
Locations: Chandler, AZ OR Reston, VA â€“ must be willing to go on site 2-3 days a week.

This opportunity is a long-term contract (Several years).
 SOFTWARE TEAM TECH STACK:
 This project is made up of several languages. Multi-language project. Each domain has its own language. We write services that communicate over Kafka or REST and what you use depends on where you are in the application. Fast API for REST API, python. On the MPS side it is similar but some legacy code from commercial which is Pearl that has been developed over a decade. There is a CI/CD pipeline that runs through Gitlab that deploys automatically to Kubernetes cluster and use rancher and a provided PAAS. Primarily on the ground side we are using java spring boot for the development and some cloud players here and there. A lot of back end managing hardware and collecting metrics. Handful of UIâ€™s and written in JavaScript. Sourcing information from other places so Grafana is used as well. Do some unit testing but have a dedicated integration team. For test automation â€“ they are early in their journey. We are still shaping the unit testing.
  Must Haves:
Bachelor of Science degree in Computer Science, or other engineering discipline or 4 additional years in lieu of degree
12+ years software programming and/or support experience with mission planning and/or network management
Current Active Security Clearance or ability to obtain a DOD clearance
Tech Stack:
DevOps Tools: GitLab (main pipeline tool), Artifactory.
CI/CD Pipeline: Expertise in creating and maintaining pipelines using GitLab with YAML and shell scripting.
Containerization: Experience with Docker, Kubernetes, and Podman
Cloud: Familiarity with Terraform and EKS.
Deployment: Ability to create, debug, and maintain deployment aspects in Kubernetes and Docker.
Automation: Building frameworks for automation, standing up in Rancher, and fixing issues.


Responsibilities:
Pipeline Execution: Not just running pipelines but fixing problems and ensuring smooth execution.
Cross-Team Interaction: Heavy interaction with support teams, app teams, and customers to create, architect, and deploy DevOps workflows.
Long-Term Goals: Aim to become the DevOps expert lead, owning the DevOps processes and backfilling current team needs.


Compensation:
$59/hr to $66/hr.

Exact compensation may vary based on several factors, including skills, experience, and education.

Employees in this role will enjoy a comprehensive benefits package starting on day one of employment, including options for medical, dental, and vision insurance. Eligibility to enroll in the 401(k) retirement plan begins after 90 days of employment. Additionally, employees in this role will have access to paid sick leave and other paid time off benefits as required under the applicable law of the worksite location.

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4264227380,https://www.linkedin.com/jobs/view/4264227380,seetha.pdf,Unknown,2025-07-09 19:29:50.286288,Problem in Easy Applying,"Seems like stuck in a continuous loop of next, probably because of new questions.",Easy Applied,4264227380 - Failed at questions - 2025-07-09 19:29:47.629987.png
4259896332,https://www.linkedin.com/jobs/view/4259896332,Pending,Unknown,2025-07-09 19:30:18.161266,Asking for Security clearance,"
About the job
Job Title: DevOps Engineer
Locations: Annapolis Junction, MD
Job Type: Onsite
 Full-Time/Permanent job position
  JOB DESCRIPTION:

 Senior level candidate needed with strong Kubernetes, Terraform and AWS. Top Secret/SCI Clearance/FSP Required.
   Role Responsibilities:
Provide optimization and automation across multiple platforms and applications.
Drive consistency for deployment and build processes.
Utilize modern tools like Jenkins, Docker, Kubernetes, Amazon Web Services, Ansible, Terraform, Python, Linux, and much more.
You have worked with teams before on large and demonstrable projects (preferably built on with AWS and familiarity with scripting languages such as Bash, Ruby, Python).
Familiarity with containerization tools like Docker and Kubernetes.
You understand how to set up a CI/CD pipeline.
You understand Agile software development and DevOps practices and can work closely with your peers other mid-to large size teams.
Focus on understanding customers' needs and translating those needs from product specifications into functional, production ready code and infrastructure.
The following skills are required: AWS (S3, VPCs & Networking, EC2, ECS/EKS). Containerization (Docker, k8s, Registries). IaC (Terraform/Cloud Formation). CI/CD (Jenkins/ GitHub Actions).
You are self-driven, self-learner willing to share knowledge and participate actively in a team environment.
CWIP IAT Level I required.
Experience above is required with a bachelorâ€™s degree.
4 years of additional experience can be substituted for a bachelorâ€™s degree.
4 years less experience is ok for candidates with a graduate degree.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4255252675,https://www.linkedin.com/jobs/view/4255252675,Pending,Unknown,2025-07-09 19:30:44.630895,Asking for Security clearance,"
About the job
Company Description

45Press has been in business is 2012. We are a WordPress agency, full stack and hosting/DevOps. We have hosting customers that are not dev customers. We are a well rounded team. We work with customers in higher ed, b2b, entertain, music, and governement.

Role Description

This is a full-time on-site role for a DevOps Engineer at 45PRESS, Inc. located in Cleveland, OH. The DevOps Engineer will be responsible for managing and automating infrastructure, developing and maintaining CI/CD pipelines, performing system administration tasks, and ensuring the stability and scalability of Linux servers.

Qualifications

Experience with Infrastructure as Code (IaC) practices
Proficiency in Software Development
Knowledge of Continuous Integration (CI) practices
Strong System Administration skills
Proficiency with Linux operating systems
Excellent problem-solving skills and attention to detail
Ability to work collaboratively in an on-site team environment
Bachelor's degree in Computer Science, Information Technology, or related field is preferred
Experience with cloud platforms such as AWS, Azure, or Google Cloud is a plus
gov clearance and clean background check is requirement
full stack is plus
ml/ai/rag/mcp is a plus. STRONG PREFERENCE IN YOUNGSTOWN,WARREN, or 30-45 radius of Canfield OH

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4264126298,https://www.linkedin.com/jobs/view/4264126298,Pending,Unknown,2025-07-09 19:30:47.101011,Found a Bad Word in About Job,"
About the job
ðŸŒŸ Principal DevOps Engineer with AI 

Full-Time | Hybrid (3 Days Onsite â€“ New Jersey) | No Visa Sponsorship

Join a Cutting-Edge AI & DevOps Innovation Team
We're seeking a Principal DevOps Engineering Specialist with AI to play a pivotal role in advancing Large Language Model (LLM) capabilities within a modern DevOps ecosystem. This hybrid, full-time position (3 days onsite in New Jersey) is ideal for a hands-on, highly skilled engineer passionate about GenAI, automation, and AI-driven development practices.

ðŸ” What You'll Do
As a core member of the engineering team, you'll:
Drive enhancements to our DevOps processes including automation of requirements gathering, code generation, and test creation.
Design, implement, and refine LLM-based prompts to guide GenAI outputs that are accurate, relevant, and secure.
Contribute to the development of internal tools and frameworks that support software delivery and improve team productivity.
Work collaboratively across cross-functional development teams and third-party vendors to ensure smooth integration and scalable systems.
Stay ahead of the curve by exploring new GenAI techniques, tools, and models to drive innovation.
Lead initiatives in documentation, automation testing, and process reliability.

ðŸ’¡ Tech Youâ€™ll Work With
LLMs (Multiple models)
Microsoft Azure & Azure DevOps Server
GitHub Copilot
Bash, Ansible (Automation tools)
CI/CD, Infrastructure as Code, DevOps frameworks
Service-Oriented Architectures (SOA)

âœ… Your Background
Proven experience improving DevOps pipelines and implementing automation strategies.
Hands-on experience designing prompts and workflows using Generative AI / LLMs.
Knowledge of cloud platforms, especially Azure.
Familiarity with modern software development methodologies and scripting tools.
Strong collaboration and documentation skills.
At least 4â€“6 years of relevant experience and a BA/BS in Computer Science or related field.
2+ years working with LLMs in a development or automation context.

ðŸš« Please Note
No Visa Sponsorship Available â€“ You must be authorized to work in the U.S.
No C2C / No Third-Party Vendors

ðŸ¢ About Hirobe Limited
Expert recruitment, powered by a deep understanding of the Adobe Experience Cloud.
Since 2018, we've been connecting the best Adobe professionals with leading organizations, giving us a unique insight into both the technology and the talent. What truly sets us apart is our specialized market knowledge, tailored service, and unwavering commitment to finding the ideal fit for everyone involved.

ðŸ“¬ Ready to Apply?
This is an urgent hire!
ðŸ“§ Send your resume to: raj@hirobe.io
ðŸ“ž Or call: +1 917-512-8959

Letâ€™s redefine whatâ€™s possible with AI and automationâ€”together.

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4263384516,https://www.linkedin.com/jobs/view/4263384516,Pending,Unknown,2025-07-09 19:30:55.716865,Found a Bad Word in About Job,"
About the job
Job Title: Azure Data Engineer
Job Summary:
We are seeking a highly skilled and motivated Azure Data Engineer with expertise in Databricks, Azure Data Warehouse, Power BI, and data modeling using Erwin. The successful candidate will be responsible for designing, developing, and maintaining scalable data pipelines and analytics solutions that empower data-driven decision-making across the organization.
Key Responsibilities:
Design and implement scalable data pipelines and dimensional models (Star Schema).
Develop and optimize enterprise data warehouse models using Erwin, with Databricks as the data repository.
Extract and structure master data-based dimensions from existing Power BI dashboards.
Collaborate with data architects, analysts, and business stakeholders to gather requirements and translate them into robust technical solutions.
Ensure data quality, integrity, and governance across all data platforms.
Monitor, troubleshoot, and optimize data workflows and performance.
Implement CI/CD practices for efficient and reliable data pipeline deployments.
Required Skills and Qualifications:
Minimum of 5 years of experience in data engineering, with a strong focus on Power BI, Databricks, Star Schema, and Erwin.
Proficiency in Python and Apache Spark for data transformation and analysis.
Deep understanding of data modeling techniques and data warehouse architecture.
Experience with version control systems (e.g., Git) and DevOps methodologies.
Strong analytical, problem-solving, and communication skills.
Preferred Qualifications:
Microsoft Certified: Azure Data Engineer Associate or equivalent certification.
Experience with Delta Lake, Lakehouse architecture, and real-time data processing.
Familiarity with enterprise data governance tools and best practices.
Knowledge of the Downstream Oil & Gas industry is a plus.

**NO C2C Option - NO Sponsorship/MUST be eligible to work in the US***

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4264806351,https://www.linkedin.com/jobs/view/4264806351,Pending,Unknown,2025-07-09 19:31:00.114506,Required experience is high,"
About the job
About the end client:- 
operates one of the world's largest energy delivery systems, constantly at the forefront of innovations in developing technology. If youâ€™re up to the challenge, then take a chance at this rewarding opportunity!

JOB DESCRIPTION
We are looking for a skilled Azure DevOps Engineer with experience in Copado and Azure DevOps to support the secure and efficient delivery of software solutions in the utility and energy industry.
This role is ideal for someone passionate about automation, infrastructure as code, and secure software delivery in a highly regulated environment.
You'll work closely with cross-functional teams to ensure the reliable and efficient deployment of applications that power modern energy infrastructure.

Qualifications
Bachelor's Degree in Computer Science, Engineering, or a related field.
10+ years of experience in DevOps or related roles.
Hands-on experience with Azure DevOps, CI/CD tools, and scripting languages (e.g., PowerShell, Bash, Python).
Strong knowledge of IaC tools (Azure DevOps.).
Familiarity with Agile methodologies and project management tools.
Experience in the utility or energy industry is highly preferred.
Familiarity with regulatory compliance frameworks (e.g., NERC CIP, ISO 27001).

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4264748084,https://www.linkedin.com/jobs/view/4264748084,Pending,Unknown,2025-07-09 19:31:13.116051,Found a Bad Word in About Job,"
About the job
Hello,
This is Ryan from Infojini Inc.
 I hope you are healthy and safe!
 Please find the below requirement and let me know if you wish to apply.
 Please send me your updated resume and expected hourly pay rate if you are interested and available!
You can also call me at 551-220-5311 to know more!
Title- Senior Cloud Security Engineer
ONLY W2, NO C2C
Location- HYBRID- 3 days on site, 3 days remote- Columbia, SC
NON LOCAL APPLICANTS NEED TO RELOCATE TO SC POST OFFER
Duration- Long Term Cotract- 1 year to begin with
Client- Direct State Client
Please make sure this is clearly listed on the first page of the resume
MUST HOLD AWS OR AZURE CERTIFICATION

If the candidate does not have this certification please do not proceed with the submission as this is required in addition to the other education and skills.

SCOPE OF THE PROJECT:
The Senior Cloud Security Engineer will collaborate with Product Owners, Applications Owners, Network and Security teams to design and deliver optimal solutions for large-scale app and infrastructure cloud solutions. Candidates should possess proficiency in both public and hybrid cloud models. Exceptional consulting skills and the ability to clearly derive desired states from customer requests are essential. The role also requires the ability to work with delivery teams. Importantly, this role is hands-on in delivery, requiring active involvement in the implementation and execution of security solutions.

DAILY DUTIES / RESPONSIBILITIES:
â€¢ Design and deploy highly available, scalable, and secure cloud infrastructure and apps with a focus on AWS and Azure Cloud.
â€¢ Design and develop automation to build cloud security accelerators and IP.
â€¢ Experience with Security Operations teams to build and maintain SIEM, SOAR and XDR technology.
â€¢ Analyze and resolve configuration issues in development, test, and production environments.
â€¢ Familiar with major security compliance frameworks and building effective monitoring, logging, and auditing of production systems to ensure compliance with mandated compliance policies (e.g., NIST, HIPAA, PII, SOX, PCI, CMMC, CUI, etcâ€¦..).
â€¢ Experience implementing secure (zero trust) infrastructure in cloud.
â€¢ Security experience with state and local government customers
â€¢ Open to Travel as needed to customer engagements

REQUIRED SKILLS (RANK IN ORDER OF IMPORTANCE):
5-10 YEARS' EXPERIENCE IN INFORMATION SECURITY AND ENGINEERING

EXPERIENCE DESIGNING AND MAINTAINING INFRASTRUCTURE IN AWS/AZURE

CLOUD SECURITY DESIGN, OPERATIONS, AND AUTOMATION EXPERIENCE

EXCELLENT COMMUNICATION SKILLS, BOTH WRITTEN AND VERBAL

DOCUMENTATION OF SECURITY TOOLS, DEPLOYMENT CONFIGURATION, INCIDENT REPORTS, ETC

PREFERRED SKILLS (RANK IN ORDER OF IMPORTANCE):
EXPERIENCE DEPLOYING, CONFIGURING AND MAINTAINING CLOUD VM SERIES PALO ALTO FIREWALLS IN AWS AND AZURE
PRISMA CLOUD
FORESCOUT
CISCO UMBRELLA
PALO ALTO FIREWALLS
F5 LOAD BALANCING/FIREWALL

REQUIRED EDUCATION: BACHELORâ€™S DEGREE OR EQUIVALENT EXPERIENCE.
MUST HOLD AWS OR AZURE CERTIFICATION

PREFERRED CERTIFICATIONS: CISSP - CERTIFIED INFORMATION SYSTEMS SECURITY PROFESSIONAL
SC100 - MICROSOFT CYBERSECURITY ARCHITECT
AWS CERTIFIED CLOUD SOLUTIONS ARCHITECT, PRISMA CERTIFIED CLOUD SECURITY ENGINEER, PCNSA, PCNSE

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4260752247,https://www.linkedin.com/jobs/view/4260752247,Pending,Unknown,2025-07-09 19:31:15.461042,Required experience is high,"
About the job
Job Title: DevOps Engineer
Experience: 8â€“10 Years
Location: Chicago, IL

Job Description:
We are seeking a Senior DevOps Engineer responsible for the analysis, design, and development of scalable, secure software solutions across the full software development lifecycleâ€”from conception to deployment. The ideal candidate will be proficient in both front-end and back-end technologies, with a strong emphasis on Java, Spring Boot, Azure (AKS), and JUnit/Gradle. You will be a key contributor to a cross-functional Agile team, driving innovation and automation in a cloud-native environment.

Key Responsibilities:
Design, develop, and maintain scalable microservices and cloud-native applications.
Implement CI/CD pipelines and automate infrastructure using DevOps best practices.
Collaborate with developers, QA, and operations teams to ensure seamless deployment and monitoring.
Optimize system performance, reliability, and security across environments.
Troubleshoot and resolve issues in development, test, and production environments.

Required Skillsets:
Programming Languages: Java (1.8, 21), Python, Shell scripting
Frameworks & Tools: Spring Boot, Spring Batch, Spring Integration, JUnit, Gradle, Maven
Cloud & DevOps: Azure, AKS, PCF, Docker, GitHub, CI/CD pipelines
Databases: Oracle, SQL, Cosmos DB, MarkLogic, NoSQL
Big Data & Messaging: Azure DataBricks, ADF, Kafka
Operating Systems: Unix/Linux
IDEs & Platforms: IntelliJ, Eclipse
Architecture: Microservices design and implementation

Preferred Qualifications:
Experience with Agile/Scrum methodologies
Strong problem-solving and communication skills
Ability to work independently and in a team-oriented environment

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4262160401,https://www.linkedin.com/jobs/view/4262160401,Pending,Unknown,2025-07-09 19:31:19.993552,Required experience is high,"
About the job
Optomi, in partnership with a global leader in IT infrastructure services, is seeking a Lead DevOps Engineer to drive the future of automation and infrastructure in an Azure-native environment.

This is a high-impact, hybrid technical/strategic role where youâ€™ll lead intelligent automation initiatives, embed AI capabilities into CI/CD and infrastructure workflows, and guide a senior DevOps engineer. Youâ€™ll collaborate across architecture, platform, and software teams to modernize operations and scale SaaS-focused solutions using Azure Fabric and Microsoft-native tools.


Key Responsibilities
Architect and scale CI/CD pipelines using GitHub Actions and Azure DevOps tools, embedding AI/ML models for intelligent testing, deployment, and observability.
Lead implementation of predictive monitoring and alerting with Dynatrace and Azure Monitor to enable self-healing systems.
Define and execute infrastructure automation strategies with Terraform, aligning with governance and long-term scalability goals.
Mentor and guide DevOps engineers, fostering autonomy, technical excellence, and innovation.
Collaborate with architecture, data, and engineering teams to align automation strategy with enterprise-wide SaaS and Azure Fabric adoption.
Evaluate and adopt emerging Azure AI services (e.g., Azure Machine Learning, Cognitive Services) to enhance infrastructure intelligence.
Serve as liaison across teams, driving DevOps priorities, best practices, and automation-first culture.

Core Requirements
7+ years in DevOps or cloud engineering with hands-on technical leadership experience.
Deep expertise with GitHub Actions or similar CI/CD tooling in enterprise Azure environments.
Advanced experience with Terraform for large-scale infrastructure automation.
Proven knowledge of Dynatrace, Azure Monitor, or similar observability tools with AI/ML-based alerting strategies.
Demonstrated ability to mentor engineers, influence technical culture, and lead DevOps transformation.
Hands-on experience with Azure AI services and embedding intelligence into infrastructure or DevOps workflows.
Excellent communication skillsâ€”able to work cross-functionally and drive clarity from ambiguity.

Preferred Qualifications
Experience with Azure Fabric and transitioning to SaaS-native architectures.
Background in both infrastructure and software engineering with an SRE/DevOps mindset.
Familiarity with data platform automation and scaling distributed systems.
Ability to balance architectural governance with flexibility and innovation.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4264742356,https://www.linkedin.com/jobs/view/4264742356,Pending,Unknown,2025-07-09 19:31:22.313290,Required experience is high,"
About the job
Job Title: Senior DevOps Engineer
Location: 4440 Rosewood Dr, Pleasanton, CA â€“ 5 days Onsite role, preferred local resources first
Employment Type: Fulltime â€“ No H1B Transfers, Independent resources are welcome to apply (USC, GC, EADâ€™s)
Position Open: 2

Key Skills: Building CI/CD Pipelines, Automation, Shell scripting, Jenkins, Dockers, Azure Environments, AKS, GitHub Actions, APIâ€™s and Microservices.

Job Description:
At least 8 years of experience as an DevOps Engineer.
Design, implement, and maintain GitHub Actions (GHA) pipelines (IOS, Android and Java) to automate CI/CD workflows.
Manage and support artifact deployments across staging and production environments.
Diagnose and resolve issues in existing GHA pipelines to ensure uninterrupted integration and delivery processes.
Develop and maintain supporting infrastructure and artifacts, including provisioning and modifying Azure/OCI databases.
Stay current with emerging technologies and demonstrate a willingness to learn and adopt new tools and frameworks as needed.
Assist in managing SSL/TLS certificates and ensure adherence to PCI and SOX compliance requirements.
Provide support and troubleshooting expertise for testing tools, including JUnit and SonarQube.
Identify and resolve performance issues within Azure Kubernetes Service (AKS) environments.
Lead the migration of legacy Jenkins jobs to GitHub Actions, ensuring continuity and improvement of CI/CD practices.
Troubleshoot and resolve build and performance issues on macOS systems, including working with Xcode and related toolchains.

Special Skills:
Extensive experience with GitHub Actions (GHA), ArgoCD, and Chartis for automating deployments of Java micro services on Azure Kubernetes Service (AKS)
Proven ability to configure and optimize iOS and Android build pipelines using GitHub Actions.
Experience migrating and managing workloads on AKS, with strong understanding of container orchestration.
Proficient with Docker and containerization technologies.
Familiar with infrastructure-as-code and configuration management using Ruby and Opscode Chef.
Strong shell scripting skills for automating infrastructure and operational tasks
Experience using JMeter and BlazeMeter for load and performance testing
Knowledge of SonarQube for static code analysis and quality enforcement.
Exposure to Java and Kotlin in the context of supporting mobile and backend CI/CD pipelines.
Working knowledge of iOS/Xcode build systems and performance debugging on macOS.
Familiarity with Google Cloud Platform (GCP), particularly for centralized logging and monitoring.
Experience with observability tools including OpenTelemetry and Prometheus for monitoring and alerting.
Comfortable working in Linux and macOS environments, with strong troubleshooting skills on both platforms.
Strong ability to analyze CPU and memory usage within Azure Kubernetes Service (AKS), enabling data-driven adjustments to resource allocation for optimal performance and cost efficiency

Experience required 8 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4255252675,https://www.linkedin.com/jobs/view/4255252675,Pending,Unknown,2025-07-10 10:33:36.571148,Asking for Security clearance,"
About the job
Company Description

45Press has been in business is 2012. We are a WordPress agency, full stack and hosting/DevOps. We have hosting customers that are not dev customers. We are a well rounded team. We work with customers in higher ed, b2b, entertain, music, and governement.

Role Description

This is a full-time on-site role for a DevOps Engineer at 45PRESS, Inc. located in Cleveland, OH. The DevOps Engineer will be responsible for managing and automating infrastructure, developing and maintaining CI/CD pipelines, performing system administration tasks, and ensuring the stability and scalability of Linux servers.

Qualifications

Experience with Infrastructure as Code (IaC) practices
Proficiency in Software Development
Knowledge of Continuous Integration (CI) practices
Strong System Administration skills
Proficiency with Linux operating systems
Excellent problem-solving skills and attention to detail
Ability to work collaboratively in an on-site team environment
Bachelor's degree in Computer Science, Information Technology, or related field is preferred
Experience with cloud platforms such as AWS, Azure, or Google Cloud is a plus
gov clearance and clean background check is requirement
full stack is plus
ml/ai/rag/mcp is a plus. STRONG PREFERENCE IN YOUNGSTOWN,WARREN, or 30-45 radius of Canfield OH

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4255193878,https://www.linkedin.com/jobs/view/4255193878,Pending,Unknown,2025-07-10 10:33:38.888232,Asking for Security clearance,"
About the job
About Concept Plus
Concept Plus is a growing consulting firm headquartered in Fairfax, VA. We are an Oracle Gold Partner, offering deep technical expertise, combined with business insights and an experienced team focused on providing technical solutions for our clients. We are proud to have been recognized as one of the ""25 Most Powerful Oracle Solution Providers"" in the area! We offer great benefits including competitive pay, comprehensive health insurance, dental and vision insurance, paid life insurance, paid time off, 11 paid holidays, bonuses, tuition reimbursement, unlimited training, and the opportunity to work in a collaborative, flexible, innovative environment! For additional information about our dynamic organization, please visit our website. at www.conceptplus.com. 

About the role
Concept Plus is seeking a DevOps Software Engineer(Cloud) to provide technical support to a federal clients' systems. The DevOps Software Engineer (Cloud) will promote state-of-the-art IT solutions, collaborating with cross-functional teams, and shared lessons learned with the team.

What you'll do
Focus on enabling container deployments and implementation of serverless architecture within AWS. Containerize applications using Docker and orchestrate them with Kubernetes or Mesos.
Design, develop, and deploy robust and scalable software solutions using object-oriented programming principles and languages like Java, JavaScript, Python, or Ruby.
Leverage AWS services(e.g., CloudFormation, EC2, Lambda, DynamoDB, and Elasticsearch) to build cloud-native applications.
Implement and maintain continuous integration and continuous delivery (CI/CD) pipelines using tools like Chef, Puppet, Docker, Jenkins, and Ansible.
Automate tasks and processes using scripting languages like Bash.
Manage source code effectively using tools like GitHub or GitLab.
Work with relational databases such as RDS, Oracle, and Postgres to store and retrieve data.
Utilize log analytics tools to monitor and troubleshoot system performance and resolve technical issues.
Stay up to date with the latest technologies and industry trends.
Contribute to a positive and collaborative team environment.

Qualifications
5+ years of experience as a DevOps Software Engineer
Object Oriented Programming experience 
Experience with AWS(e.g., CloudFormation, EC2, Lambda, DynamoDB, and Elasticsearch)
Experience with relational databases (e.g., RDS, Oracle, Postgres)
Experience in continuous integration technology (e.g., Chef, Puppet, Docker, Jenkins, Ansible)
Experience with interacting in a Linux environment (ex- Bash scripting)
Security+ CE or CCNA Security certification required
Active Secret Clearance or the ability to obtain a Secret Clearance

Preferred
Bachelor's Degree
Excellent communication and presentation skills -- with users, technical teams, and senior management
Versatile and enthusiastic to take on new problems
Comfortable working in a fast-paced environment
Comfortable managing competing priorities and able to bring order to ambiguous scenarios
Experience in an Agile environment (JIRA, Confluence)
AWS DevSecOps Certifications or willingness to pursue

Concept Plus is an Affirmative Action/Equal Opportunity Employer. As such, we will give your application full consideration without regard to your race, color, religion, sex, age, national origin, disability, veteran status, sexual orientation, gender identity, or any other classification protected by federal, state, or local law.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4257358150,https://www.linkedin.com/jobs/view/4257358150,Pending,Unknown,2025-07-10 10:33:41.325264,Required experience is high,"
About the job
Title: Devops Engineer (2 Roles)
Location: Jacksonville, Alpharetta, Berkley Heights (5 Days Onsite)
Experience: 8 â€“ 10 Years
Job Description:
Experience interacting with customers and appropriate representatives to analyze, validate, specify, verify, document and manage solution requirements. 
4+ Experience in public/private cloud environments (AWS/Azure/Google Cloud) 
Experience in the complete SDLC cycle including associated deployment methodologies, QA processes, and performance tuning efforts.
Experience in Automation/Continuous Integration (Terraform, Ansible, etc.)
Concurrent Versioning Software (GitLab, GitHub) 
Working knowledge of networking protocols and components including TCP/IP, DNS, CIFS. 
Experience designing and implementing High Availability and Disaster Recovery solutions. 
Experience working on a matrix team consisting of a variety of Infrastructure Engineering groups, as well as Development groups and Client Relationship groups. 
Willingness and capability of quickly learning new technologies required for accomplishing project goals.

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4264126298,https://www.linkedin.com/jobs/view/4264126298,Pending,Unknown,2025-07-10 10:33:43.740528,Found a Bad Word in About Job,"
About the job
ðŸŒŸ Principal DevOps Engineer with AI 

Full-Time | Hybrid (3 Days Onsite â€“ New Jersey) | No Visa Sponsorship

Join a Cutting-Edge AI & DevOps Innovation Team
We're seeking a Principal DevOps Engineering Specialist with AI to play a pivotal role in advancing Large Language Model (LLM) capabilities within a modern DevOps ecosystem. This hybrid, full-time position (3 days onsite in New Jersey) is ideal for a hands-on, highly skilled engineer passionate about GenAI, automation, and AI-driven development practices.

ðŸ” What You'll Do
As a core member of the engineering team, you'll:
Drive enhancements to our DevOps processes including automation of requirements gathering, code generation, and test creation.
Design, implement, and refine LLM-based prompts to guide GenAI outputs that are accurate, relevant, and secure.
Contribute to the development of internal tools and frameworks that support software delivery and improve team productivity.
Work collaboratively across cross-functional development teams and third-party vendors to ensure smooth integration and scalable systems.
Stay ahead of the curve by exploring new GenAI techniques, tools, and models to drive innovation.
Lead initiatives in documentation, automation testing, and process reliability.

ðŸ’¡ Tech Youâ€™ll Work With
LLMs (Multiple models)
Microsoft Azure & Azure DevOps Server
GitHub Copilot
Bash, Ansible (Automation tools)
CI/CD, Infrastructure as Code, DevOps frameworks
Service-Oriented Architectures (SOA)

âœ… Your Background
Proven experience improving DevOps pipelines and implementing automation strategies.
Hands-on experience designing prompts and workflows using Generative AI / LLMs.
Knowledge of cloud platforms, especially Azure.
Familiarity with modern software development methodologies and scripting tools.
Strong collaboration and documentation skills.
At least 4â€“6 years of relevant experience and a BA/BS in Computer Science or related field.
2+ years working with LLMs in a development or automation context.

ðŸš« Please Note
No Visa Sponsorship Available â€“ You must be authorized to work in the U.S.
No C2C / No Third-Party Vendors

ðŸ¢ About Hirobe Limited
Expert recruitment, powered by a deep understanding of the Adobe Experience Cloud.
Since 2018, we've been connecting the best Adobe professionals with leading organizations, giving us a unique insight into both the technology and the talent. What truly sets us apart is our specialized market knowledge, tailored service, and unwavering commitment to finding the ideal fit for everyone involved.

ðŸ“¬ Ready to Apply?
This is an urgent hire!
ðŸ“§ Send your resume to: raj@hirobe.io
ðŸ“ž Or call: +1 917-512-8959

Letâ€™s redefine whatâ€™s possible with AI and automationâ€”together.

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4260717006,https://www.linkedin.com/jobs/view/4260717006,Pending,Unknown,2025-07-10 10:33:52.356687,Found a Bad Word in About Job,"
About the job
Job Role: OpenShift Administrator (with Linux administration, Automation & DevOps Expertise)
Work Locations:(Pittsburgh-PA/Strongsville-OH/Birmingham-AL/Dallas-TX/Phoenix, AZ)

IMMEDIATE HIRING

Job Summary:

Clarium is seeking a skilled OpenShift Administrator with a strong background in Linux system administration, scripting, automation and DevOps practices. The ideal candidate will be responsible for maintaining and supporting RedHat OpenShift clusters in production environment, providing 24*7 support, incident triage, and participating in release activities. You will work closely with cross-functional teams to ensure system reliability, scalability, and performance.

 
Key Responsibilities:
  Administer and maintain RedHat OpenShift clusters in production and non-production environments. Upgrade Clusters
Perform Linux system administration, troubleshooting, patching
Monitor cluster health, manage node scalability, resource optimization and ensure cluster availability (No downtime)
Troubleshoot and resolve issues related to infrastructure, network, pods, containers, and services
Participate in 24x7 production support (on-call rotation), handle critical incidents, and ensure minimal platform downtime
Develop and maintain shell/python scripts for automation of routine tasks and monitoring
Implement and support CI/CD pipelines using tools like Jenkins, GitOps Approach, or ArgoCD
Collaborate with development and release teams to support deployment and release activities
Create and maintain documentation for systems, processes, and standard operating procedures (SOPs)
Ensure compliance with security, backup, and disaster recovery policies
Platform monitoring using Dynatrace


Required Qualification:

â€¢ 5+ years of hands-on experience with RedHat OpenShift (v4.x preferred).
â€¢ Strong proficiency in Linux (RedHat Linux, CoreOS) system administration.
â€¢ Solid experience in Shell scripting, bash, and/or Python.
â€¢ Knowledge of DevOps tools such as Jenkins, Git, Ansible, Helm, Docker, and GitOps workflows.
â€¢ Understanding of container orchestration, pod scheduling, and cluster level debugging.
â€¢ Experience working in production support environments with on-call responsibilities
â€¢ Familiarity with monitoring and logging solutions such as Prometheus, Grafana, ELK, FluentD.
â€¢ Good problem-solving skills and experience in incident triage and root cause analysis

 Preferred Qualifications:

â€¢ RedHat Certified Specialist in OpenShift Administration or equivalent
â€¢ Certified Kubernetes Administration (CKA)
â€¢ Knowledge of Kubernetes, Istio, or service mesh technologies
â€¢ Experience with Azure cloud platform
â€¢ Experience with ArgoCD workflows


Work Authorization:

Must be authorized to work in the United States. (US Citizen/GC/H1B/GC EAD)

Work Environment:

Fast-paced, enterprise-grade infrastructure with high uptime requirements
Expectation to work during critical incidents, maintenance windows, and release weekends.
Team collaboration across IT operations, development, QA, and DevOps

Compensation & Benefits:

Competitive salary based on experience.
Medical, dental, vision insurance.
401(k) with company match.
Paid time off and holidays.
Professional development opportunities.

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4264258655,https://www.linkedin.com/jobs/view/4264258655,Pending,Unknown,2025-07-10 10:33:54.814267,Required experience is high,"
About the job
Role : Azure DevOps Engineer
Location : California and Nevda ( Onsite) 
contract role 
Experience required : 13 years 

10+ years of experience in Cloud Engineering, DevOps, or related roles.
Strong expertise in Terraform IAC for Infrastructure as Code (IaC).
Proficiency in CI/CD tools like Jenkins, GitHub Actions, or GitLab CI/CD.
Knowledge of containerization technologies (Docker, Kubernetes).

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4257999754,https://www.linkedin.com/jobs/view/4257999754,Pending,Unknown,2025-07-10 10:34:03.231096,Required experience is high,"
About the job
Job Title : Cloud and DevOps Engineer (w/GCP)
Location : Auburn Hills, Michigan(Hybrid) 
Duration : Long term 


Job Description 
Cloud and DevOps Engineer
We are seeking a highly skilled Cloud and DevOps Engineer with a strong focus on Google Cloud Platform (GCP) to join our dynamic team. In this role, you will play a key part in designing, implementing, and maintaining scalable and efficient cloud infrastructure, as well as streamlining our DevOps processes. You will work closely with development and operations teams to automate workflows, enhance system reliability, and optimize cloud resources while following best practices in cloud security and governance. Your expertise will directly influence the speed and quality of our software delivery pipeline. 
Design, implement, and manage cloud infrastructure on Google Cloud Platform (GCP). 
Automate and optimize deployment pipelines using CI/CD tools and processes (e.g., Jenkins, GitLab CI, GitHub Actions). 
Develop and maintain Infrastructure as Code (IaC) using tools like Terraform or Google Cloud Deployment Manager. 
Work with containerization technologies like Docker and Kubernetes for efficient application delivery and scaling. 
Collaborate with development teams to ensure smooth integration of DevOps practices in the software development lifecycle.
Implement and enforce cloud security best practices across Google Cloud environments. 
Monitor cloud resources and optimize cost and performance across GCP services. 
Implement automated testing and validation within the CI/CD pipeline to ensure quality and stability of releases. 
Troubleshoot and resolve issues related to cloud infrastructure, DevOps processes, and application performance.
Required Skills:
7-8 years of experience in Cloud
Proficiency with Google Cloud Platform (GCP), including services like Google Compute Engine, Google Kubernetes Engine (GKE), Google Cloud Storage, Cloud Functions, and BigQuery. 
Strong knowledge of Infrastructure as Code (IaC), with experience using Terraform, Google Cloud Deployment Manager, or similar tools. Hands-on experience with containerization technologies such as Docker and Kubernetes. 
Familiarity with modern CI/CD tools and practices (e.g., Jenkins, GitLab CI, GitHub Actions, or similar). 
Knowledge of scripting languages such as Python, Bash, or PowerShell. 
Deep understanding of cloud networking concepts (VPC, load balancing, DNS) and cloud security best practices (IAM, service accounts, encryption). 
Experience with monitoring and logging tools like Prometheus, Grafana, ELK stack, or Stackdriver. 
Solid understanding of version control systems (Git) and the software development lifecycle. 
Expertise in cloud architecture design and implementation specifically in Google Cloud. 
Strong skills in optimizing cloud costs and performance in GCP environments. 
Proven ability to implement advanced DevOps practices such as GitOps and ChatOps. 
Experience with building and managing cloud-native applications that scale efficiently in GCP. 
Preferred Qualifications: 
Google Cloud Certified is highly desirable. 
Experience with serverless computing using services like Cloud Functions or Cloud Run. 
Familiarity with machine learning workloads on Google Cloud and managing related infrastructure.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4261960932,https://www.linkedin.com/jobs/view/4261960932,Pending,Unknown,2025-07-10 10:34:05.705570,Asking for Security clearance,"
About the job
Akkodis is seeking an Azure Cloud Engineer for a full-time position with a client located in Portland, ME. Ideally, looking for a local candidate based out of Portland, ME/Boston, MA/Dallas, TX or Chicago, IL and not someone willing to relocate.

Salary Range: $160,000 to $180,000/ Year(The salary may be negotiable based on experience, education, geographic location, and other factors)

Title: Azure Cloud Engineer
Location: Portland, ME/Boston, MA/Dallas, TX or Chicago, IL (Hybrid once in a quarter)

Job Description:
We are seeking a highly skilled and motivated Azure Cloud Engineer with a strong development background to join our dynamic engineering team. The ideal candidate will have a proven track record of designing, implementing, and managing scalable cloud infrastructure solutions on the Azure platform, leveraging Infrastructure as Code (IaC) principles and cutting-edge technologies. The Azure Cloud Engineer will play a pivotal role in driving our cloud initiatives, ensuring the reliability, security, and efficiency of our Azure cloud environments.

How you'll make an impact
Cloud Infrastructure Design and Implementation: Design and implement robust, scalable, and secure cloud infrastructure solutions on the Azure platform, adhering to industry best practices and company standards.
Infrastructure as Code (IaC): Utilize Terraform to define and manage Azure cloud infrastructure as code, ensuring consistency, repeatability, and traceability across environments.
Version Control: Manage and maintain version control systems, particularly using Git (github), to ensure codebase integrity and collaboration.
Cloud Automation: Develop and maintain automation scripts and tools using Python to streamline Azure cloud operations, deployments, and resource management tasks.

Experience you'll bring
A minimum of 7+ years of hands-on experience in designing, implementing, and managing cloud infrastructure on the Azure platform.
Proven expertise in using Terraform for Infrastructure as Code (IaC).
Strong proficiency in Python scripting for automation and Azure cloud operations tasks.
Solid understanding of Windows operating systems for cloud deployments.

Certifications:
Possess the Microsoft Certified: Azure Administrator Associate certification or higher.

Skills:
Deep understanding of Azure cloud architecture principles, networking concepts, and security best practices.
Excellent problem-solving and troubleshooting skills.
Strong communication and collaboration skills, with the ability to work effectively in a team environment.
Self-motivated and proactive, with a passion for learning and staying up-to-date with the latest Azure cloud technologies.

Additional Preferred Qualifications:
Understanding of advanced networking concepts and experience with Azure networking services such as Vnet, VPN, and Load Balancing.
Understanding of PKI infrastructure (certificates, encryption keys, and authentication protocols).
Experience with containerization technologies (e.g., Docker, Kubernetes).
Experience with CI/CD tools such as Jenkins, GitLab, Github Actions, or Azure DevOps.
Experience with logging and monitoring products such as Splunk and Datadog.
Experience with Azure cloud cost optimization strategies.

Benefits include but are not limited to:
Medical/Dental/Vision
401K
PTO/Paid Holidays
To read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy.

The Company will consider qualified applicants with arrest and conviction records in accordance with federal, state, and local laws and/or security clearance requirements, including, as applicable:
Â· The California Fair Chance Act
Â· Los Angeles City Fair Chance Ordinance
Â· Los Angeles County Fair Chance Ordinance for Employers
Â· San Francisco Fair Chance Ordinance

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4259859439,https://www.linkedin.com/jobs/view/4259859439,Pending,Unknown,2025-07-10 10:34:10.087169,Asking for Security clearance,"
About the job
The Infrastructure Engineer works on holistic engineering deliverables across different stages of the product lifecycle and determines technology patterns for the overall solution. 
  Experience with Kubernetes - OpenShift(OCP) at a platform level
Experience with any one OpenShift / K8s supported Networking CNI. (Plus if you know Calico ) 
Experience with AWS infrastructure (VPC / Security groups / EC2 / Storages / Load Balancers) 
Experience doing OCP upgrades and creating OCP Clusters
Experience of platform automation products like RH ACM / Terraform / Ansible
Responsible for incident resolution and Pager Support on a rotation basis.
Achieves product commitments (and influences others to do the same) by using informal leadership & highly developed communication skills and contributes to or led technology communities.
Uses automation, system tools, open-source solutions, observability and 'security first' principles in daily work 

Preferred Qualifications 
Kubernetes certification like - CKA or CKAD and AWS certification.
Application programming skills in Golang, Java, etc.
Understanding of k8s operator model. 
Scripting languages like shell / Helm modules or some programming experience. 
Diagraming skills such as sequence diagrams, flowcharts, etc.
Concepts of secrets/credential management for Applications and systems - such as use of HashiCorp Vault
Operators in Kubernetes/OCP

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4264250788,https://www.linkedin.com/jobs/view/4264250788,Pending,Unknown,2025-07-10 10:34:18.612362,Asking for Security clearance,"
About the job
****************Only USC / Green Card/ GC EAD/H4 EAD****************


Position: Sr. DevOps Consultant
Location: Pennington, NJ/Jersey City (Hybrid model) - 3 days onsite
Duration: 18 months
Client: Bank of America
Rate: $70-73/hr on W2. 


Responsible for defining and delivering automated credential management solutions aligning with the enterprise strategy.
Ensures the solution is fit for purpose by working with internal stakeholders as well as external subject matter experts to meet the enterprise needs for secret and credential management.
Consults with business stakeholders to clearly understand the business requirements, challenges of the solution, and finds creative solutions to meet the requirements.
Clarifies the architecture via detailed technical documentation.
Performs design and code reviews to ensure all functional requirements for a solution are sufficiently met

Responsibilities:
As our DevOps engineer, play a crucial role in bridging the gap between development and operations.
Collaborate with cross-functional teams to automate deployment processes and monitoring of infrastructure
Automate the next generation of Privilege Access Vaulting solution with a focus on high-availability and resiliency
Develop and maintain automated solutions leveraging Terraform, Jenkins, Ansible, Git
Managing and optimizing cloud infrastructure on platforms like Azure and AWS
Conducting proactive monitoring and troubleshooting of all systems
Maintaining up-to-date documentation on processes and configurations

Required Qualifications
7+ years of DevOps experience
Strong experience in Ansible, Jenkins, Terraform
Experience taking manual tasks/processes and automated at the enterprise level.
Demonstrated expertise of scripting via common solutions such as Python and PowerShell.
Exposure to working in environment that is heavy on operations technology (automation, optimization, rapid development)
Demonstrated expert level of RHEL Linux OS and LDAP Directory administration.
Solid understanding and demonstrable expertise delivering Linux based automated solutions at enterprise scale.
Solid understanding of infrastructure (hardware, network, storage).
Good understanding of agile methodology and associated toolset (Jira, Bitbucket, Jenkins, etc.)

Desired Qualifications
Previous experience with credential vaulting products such as: CyberArk, Centrify, HashiCorp Secrets Vault.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4257044706,https://www.linkedin.com/jobs/view/4257044706,Pending,Unknown,2025-07-10 10:34:26.858776,Found a Bad Word in About Job,"
About the job
Swoon is actively hiring a Senior Site Reliability Engineer (SRE) to join the team! 

What will your day-to-day will look like? 
Lead the planning, development, and maintenance of infrastructure 
Create a plan and roadmap for monitoring improvements 
Develop a roadmap and execute your plan 
Design and implement secure, scalable, cost-effective, cutting-edge infrastructure that meet business requirements 
Develop and implement IAC to create scalable infrastructure architectures 
Maintain clear, concise, ad accurate documentation that details our systems, design decisions, and operational procedures including runbooks and system-wide architecture diagrams 

Whatâ€™s Required / Technical Skills 
5+ years of experience in an SRE/DevOps with a focus on supporting high availability distributed productions systems 
Strong cloud experience â€“ AWS, Azure, or GCP 
2+ years of experience with containerization including Docker or Kubernetes 
Shell scripting 

Education / Certifications 
Bachelorâ€™s Degree Required

Great Things about the Company 
Make a measurable impact and support the growth of the company 
Vibrant on-site culture 
Excellent benefits including tuition credits for you and your family 
Turning ideas into real-world impact 
Voted 100 Best Arizona Companies to work for 10 years running! 

What else should you know? 
Contract â€“ 6-month contract to hire 
Hybrid Role â€“ 3 days a week on-site â€“ 2 weeks work from home 
Location: Tempe, AZ 
Hourly Rate 
US Citizen or Permanent Resident/Green Card Holder are required â€“ (Sponsorship and C2C is NOT available currently or in the future) 


Whatâ€™s Next?
Apply Now! -- Email questions to Kathryn.Jackson@swoontech.com

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4260751210,https://www.linkedin.com/jobs/view/4260751210,Pending,Unknown,2025-07-10 10:34:42.291316,Required experience is high,"
About the job
Our client is currently seeking a Senior DevOps Engineer 

Overview
In this role, you will be responsible for the design, development, and deployment of scalable and secure software solutions across the full software development lifecycle. You will work closely with both front-end and back-end technologies, with a strong focus on Java, Spring Boot, Azure (AKS), and DevOps practices.
You will contribute to building robust microservices, integrating cloud-native solutions, and ensuring continuous integration and delivery pipelines are efficient and secure. This role follows Agile development methodologies and requires strong collaboration and problem-solving skills.
Minimum Qualifications
8â€“10 years of experience in software development and DevOps engineering.
Proficiency in Java (1.8, 21) and Spring Boot frameworks.
Experience with cloud platforms, especially Azure and Azure Kubernetes Service (AKS).
Strong understanding of DevOps principles, CI/CD pipelines, and infrastructure automation.
Hands-on experience with Docker, GitHub, and Gradle/Maven.
Familiarity with Unix/Linux environments and shell scripting.
Experience with SQL and NoSQL databases (e.g., Oracle, Cosmos DB, MarkLogic).
Knowledge of microservices architecture and cloud-native design patterns.
Preferred Qualifications
Experience with Azure DataBricks, Azure Data Factory (ADF).
Familiarity with Kafka, Python, and JUnit for testing.
Exposure to Spring Integration and Spring Batch.
Experience with PCF (Pivotal Cloud Foundry) or similar cloud platforms.
Proficiency with development tools such as IntelliJ or Eclipse.
Rate: $70-80/HR

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4249934042,https://www.linkedin.com/jobs/view/4249934042,Pending,Unknown,2025-07-10 10:34:44.739441,Required experience is high,"
About the job
We are
At Synechron, we believe in the power of digital to transform businesses for the better. Our global consulting firm combines creativity and innovative technology to deliver industry-leading digital solutions. Synechronâ€™s progressive technologies and optimization strategies span end-to-end Artificial Intelligence, Consulting, Digital, Cloud & DevOps, Data, and Software Engineering, servicing an array of noteworthy financial services and technology firms. Through research and development initiatives in our FinLabs we develop solutions for modernization, from Artificial Intelligence and Blockchain to Data Science models, Digital Underwriting, mobile-first applications and more. Over the last 20+ years, our company has been honored with multiple employer awards, recognizing our commitment to our talented teams. With top clients to boast about, Synechron has a global workforce of 14,500+, and has 58 offices in 21 countries within key global markets.


Our challenge
We are seeking a highly skilled and motivated Cloud Engineer with extensive experience in Databricks, Infrastructure management, Terraform, DevOps, and Azure. The successful candidate will be responsible for designing, implementing, and managing robust cloud solutions that support our business needs. This role requires a deep understanding of cloud architecture, automation, and continuous integration/continuous deployment (CI/CD) practices.


Additional Information*
The base salary for this position will vary based on geography and other factors. In accordance with law, the base salary for this role if filled within Iselin, NJ is $140k - $150k/year & benefits (see below).

The Role
Responsibilities:
Cloud Infrastructure Management:
Design and implement scalable and secure cloud infrastructure using Azure services.
Manage and optimize cloud resources to ensure high availability and performance.
Monitor and troubleshoot cloud infrastructure and services.
Databricks Expertise:
Develop and manage Databricks environments for data processing and analytics.
Implement ETL pipelines using Databricks and integrate with other data sources.
Optimize Databricks performance and cost.
Terraform Configuration:
Write and maintain Terraform scripts to automate infrastructure provisioning.
Implement infrastructure as code (IaC) practices to ensure reproducibility and consistency.
Collaborate with development teams to integrate Terraform into CI/CD pipelines.
DevOps Practices:
Implement and manage CI/CD pipelines for automated deployments and testing.
Use tools like Jenkins, GitHub Actions, or Azure DevOps for continuous integration and continuous deployment.
Ensure security and compliance in DevOps workflows.
Azure Cloud Services:
Utilize Azure services such as Azure VMs, Azure Kubernetes Service (AKS), Azure SQL Database, Azure Blob Storage, etc.
Implement and manage Azure Networking, including VNETs, VPNs, and ExpressRoute.
Manage Azure Active Directory and role-based access control (RBAC).
Collaboration and Documentation:
Work closely with cross-functional teams including developers, data scientists, and IT operations.
Document infrastructure designs, configurations, and procedures.
Provide training and support to team members on cloud technologies and best practices.


Requirements:
Bachelorâ€™s degree in Computer Science, Information Technology, or a related field.
8+ years of experience in cloud engineering with a focus on Azure.
Proven experience with Databricks and data engineering.
Strong knowledge of Terraform and infrastructure as code principles.
Hands-on experience with DevOps tools and practices.
Proficiency in scripting languages such as Python, PowerShell, or Bash.
Solid understanding of networking concepts and cloud security best practices.
Excellent problem-solving skills and the ability to work in a fast-paced environment.

 Preferred, but not required:
Certifications in Azure (e.g., Azure Solutions Architect, Azure DevOps Engineer).
Experience with other cloud platforms (AWS, Google Cloud).
Knowledge of containerization and orchestration tools like Docker and Kubernetes.
Familiarity with Big Data technologies and frameworks.
Strong communication and collaboration skills.
Ability to work independently and as part of a team.
Adaptability to learn new technologies and methodologies.

We offer:
A highly competitive compensation and benefits package.
A multinational organization with 58 offices in 21 countries and the possibility to work abroad.
10 days of paid annual leave (plus sick leave and national holidays).
Maternity & paternity leave plans.
A comprehensive insurance plan including medical, dental, vision, life insurance, and long-/short-term disability (plans vary by region).
Retirement savings plans.
A higher education certification policy.
Commuter benefits (varies by region).
Extensive training opportunities, focused on skills, substantive knowledge, and personal development.
On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses.
Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups.
Cutting edge projects at the worldâ€™s leading tier-one banks, financial institutions and insurance firms.
A flat and approachable organization.
A truly diverse, fun-loving, and global work culture.


SYNECHRONâ€™S DIVERSITY & INCLUSION STATEMENT
 Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative â€˜Same Differenceâ€™ is committed to fostering an inclusive culture â€“ promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.

All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicantâ€™s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.

Experience required 8 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4258117581,https://www.linkedin.com/jobs/view/4258117581,Pending,Unknown,2025-07-10 10:34:53.433211,Asking for Security clearance,"
About the job
W2 Only - Initial OPT who are local to Irving ,TX Can Also Apply

Title: Devops Engineer 
Duration: 6 Months + 
Location: Irving , TX 
(ONLY W2)

Job Description:
  With a focus on continual improvement, propose and lead new and improved solutions to tools (solutions), processes and skills.
Promote Git Ops practices across the Organisation.
Promote Cloud, Security and DevOps best practices and architectures with the Development Teams
Contribute to the overall body of software development knowledge, including solution options, risk, cost, and impacts.
Mentor junior developers help them grow their experience and knowledge.
Participate actively in code reviews, tool evaluations, documenting new features.
Partner with multiple software development teams to help them automate their build/deploy/publish processes and adopt the Enterprise CI/CD platform.
Help extend the enterprise CI/CD platform to support new tools and technologies in ways that facilitate adoption, develop high quality and well-designed software, with automated tests.
Skill Requirements:
Must Haves:
5+ years in Cloud software development (AWS & Azure are the preferred cloud environments), with a focus on Infrastructure-as-Code tools like Terraform and AWS CloudFormation.
4+ years in software development creating applications and/or REST API services, with languages such as Java/Groovy, Python or NodeJS and test automation, such as JUnit or Mocha.
Ability to discuss and promote best practices in software development and cloud infrastructure.
Experience in implementing CI and CD process and tools like Jenkins/AWS Code pipeline, Jfrog.
Experience with log and metric consolidation tools like New Relic, ELK, Logz.io, Opensearch Prometheus and Grafana.
Experience in deploying and troubleshooting applications in containers, such as Docker and Kubernetes, AWS Elastic Kubernetes Service and/or Azure Kubernetes Service.
Experience with Build tools such as Maven, Gradle, NPM, iOS and android builds.
Experience in complex deployments like canary and blue-green deployments.
Experience in DevSecOps tools used for secret detection, vulnerability detection, container scans and experience in remediating those vulnerabilities.
Strong desire for automation and continuous learning/improvement.
Strong Debugging Skills in CI/CD and experience in handling critical incidents after hours as an on-call.
Preferred:
Cloud/DevOps/Kubernetes Certifications.
Understanding of Cloud Networking.
Experience Requirements:
5++ years of overall work experience as a DevOps/Platform Engineer.
Bachelorâ€™s Degree or equivalent education and experience

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4260296491,https://www.linkedin.com/jobs/view/4260296491,Pending,Unknown,2025-07-10 10:35:10.363807,Asking for Security clearance,"
About the job
Job title: Cloud DevOps Engineer
Location: Remote
Duration: 6+ Months Contract to Hire
 Job Description: 
The Cloud DevOps Engineer will play a pivotal role in advancing our cloud strategy. The primary focus of this role will include developing, communicating and implementing cloud Continuous integration and continuous delivery (CI/CD) pipelines which are robust and secure. This role will work closely with all stakeholders to create fully automated pipelines which support current DevOps best practices.

Responsibilities: 
- Design and implement CI/CD pipelines for Data and API workloads
- Enable security checks and policy enforcements into data pipelines (access controls, data masking, audit logging etc.)
- Collaborate with data engineers and security teams to ensure regulatory compliance (SOC2, etc.)
- Integrate static and dynamic scanning tools into build pipelines
- Monitor platform security and respond to vulnerabilities or incidents
- Manage secrets, credentials, role-based access etc. using HashiCorp Vault, SailPoint or other tools
- Implement logging, monitoring, and alerting best practices

Skills/Requirements: 
- 6+ years of experience in DataOps or DevSecOps
- 3+ years of experience with deploying Palantir Foundry or Databricks applications
- 3+ years of experience with CI/CD tools (GitHub Actions, Harness, Gitlab CI)
- Strong automation and scripting skills - Python, Bash
- Familiarity with containerization (Docker, Kubernetes)
- Experience with cloud platforms (AWS) and their security services
- Understanding of Network Security, IAM, encryption, and compliance frameworks
- Knowledge of data pipeline orchestration tools - Palantir Foundry, Airflow, etc.

Preferences 
Preferred AWS DevOps certification

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4263477876,https://www.linkedin.com/jobs/view/4263477876,Pending,Unknown,2025-07-10 10:35:14.785129,Required experience is high,"
About the job
Agility Partners is seeking a qualified AWS Cloud AI Software Engineer to fill an open position with one of our clients. This is an exciting opportunity for a hands-on technologist to drive real impact in an enterprise environment by building production-grade AI/ML systemsâ€”not just prototyping in notebooks. You'll be part of an elite team leading the development of Retrieval-Augmented Generation (RAG) pipelines, fine-tuning LLMs, and building AWS-native microservices at scale. If you've architected AI/ML services with real infrastructure, real users, and real outcomes, this builder's role is for you.

In this role, you will:
Design, develop, and maintain scalable, modular AI services using AWS components like Lambda, SageMaker, and Bedrock.
Build and optimize RAG pipelines by connecting internal datasets to inference endpoints using vector embeddings and semantic search.
Fine-tune and deploy LLM-based applications using frameworks like LangChain, Transformers, and PyTorch.
Own the end-to-end software development lifecycle, including CI/CD pipelines, infrastructure as code, and documentation.

The Ideal Candidate:
Has 10+ years of software engineering experience with strong proficiency in Python and GoLang and/or Node.js.
Brings hands-on experience developing RAG, semantic search, or LLM-based applications in production environments.
Possesses a deep understanding of AWS services, especially SageMaker, Bedrock, Lambda, and ECS.
Has contributed to open-source AI/ML/cloud projects with public, demonstrable impact (forks, stars, PRs).
Holds a Ph.D. in AI/ML/Data Science or is a named inventor on machine learning patents.
Has proven experience fine-tuning LLMs, managing datasets, and deploying ML models at scale.
Demonstrates success integrating production-ready software into release pipelines using GitHub and Terraform.

Reasons to Love It:
Work on AI projects that are grounded in real-world infrastructure and deliver enterprise value.
Join a highly skilled, mission-driven team that values innovation and impact over buzzwords.
Build reusable AI platform components that shape the future of cloud-native development.

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4264614283,https://www.linkedin.com/jobs/view/4264614283,Pending,Unknown,2025-07-10 10:35:19.509975,Required experience is high,"
About the job
A Public Sector client of ours is looking for an AWS Engineer to work on their ongoing project in the Raleigh, NC office.

Below are the additional details of this role: 

Required Qualifications:

We need a minimum of 7+ years of hands-on experience in the following.

Proficient in using various AWS services (e.g., VPC, EC2, S3, Lambda, ECS, EKS, RDS, API Gateway, Glue Crawlers, Athena) to build and manage cloud
Experience in modernizing applications, including refactoring, migration, and cloud-native development.
Knowledge of modern database technologies and strategies for migrating and modernizing database systems
Experience with DevOps practices and automation tools
Experience with data analytics, business intelligence solutions (like Power BI or AWS QuickSight )
Strong communication and collaboration skills to work with cross-functional teams, including developers, business analysts, and stakeholders.
Experience working with creating AWS ETL jobs, data pipelines with high volumes of data
AWS Certification
State Government experience


This role can be W2 or 1099/C2C and is open for anyone with valid work authorization in the US. H1B transfer candidates are more than welcome to apply for this role.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4257702677,https://www.linkedin.com/jobs/view/4257702677,Pending,Unknown,2025-07-10 10:35:47.089874,Required experience is high,"
About the job
We are looking for a skilled DevOps Engineer to join our team in Madison, Wisconsin. This position offers an exciting opportunity to work on diverse projects and utilize cutting-edge technologies. The ideal candidate will bring strong expertise in automation, infrastructure management, and continuous integration pipelines, along with a passion for optimizing system performance and reliability.

Responsibilities:
â€¢ Develop, implement, and maintain CI/CD pipelines using tools such as GitLab CI and Terraform.
â€¢ Manage infrastructure automation using Ansible and other configuration management tools.
â€¢ Monitor system performance and ensure observability using technologies like Prometheus and Loki.
â€¢ Build and maintain virtual machine images with tools like Packer.
â€¢ Collaborate with cross-functional teams to enhance system reliability and scalability.
â€¢ Analyze and mitigate risks by maintaining comprehensive risk registers.
â€¢ Provide leadership and technical guidance to support operations and project goals.
â€¢ Design and implement strategies for site reliability to optimize application performance.
â€¢ Utilize AWS technologies, including Amazon EC2, for infrastructure deployment and management.
â€¢ Drive innovation and efficiency in DevOps practices by leveraging modern tools and methodologies.
â€¢ Minimum of 7 years of experience in DevOps engineering or related roles.
â€¢ Proficiency in automation tools such as Ansible and Terraform.
â€¢ Hands-on experience with CI/CD platforms, including GitLab CI.
â€¢ Strong knowledge of observability tools like Prometheus and Loki.
â€¢ Familiarity with AWS technologies, including Amazon EC2 and related services.
â€¢ Ability to manage and support infrastructure using configuration management tools.
â€¢ Solid understanding of site reliability engineering principles.
â€¢ Experience with Agile methodologies and collaborative development environments.
Technology Doesn't Change the World, People Do.Â®

Robert Half is the worldâ€™s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.


Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go. Download the Robert Half app and get 1-tap apply, notifications of AI-matched jobs, and much more.


All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit roberthalf.gobenefits.net for more information.


Â© 2025 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking â€œApply Now,â€ youâ€™re agreeing to Robert Halfâ€™s Terms of Use.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4264611110,https://www.linkedin.com/jobs/view/4264611110,Pending,Unknown,2025-07-10 10:35:49.418252,Found a Bad Word in About Job,"
About the job
Onsite interview (Atlanta)
3 days onsite a week (Atlanta)
No H1B candidates & No C2C's

Job description:
Devops engineer (Preferably SRE)

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4262878118,https://www.linkedin.com/jobs/view/4262878118,Pending,Unknown,2025-07-10 10:36:05.749895,Asking for Security clearance,"
About the job
Here's the refined job description under the Redbeard Solutions brand:
POSITION: Senior DevOps Full Stack Developer
 WORK LOCATION: Dahlgren, VA
 JOB CATEGORY: Information Technology
 JOB TYPE: Full-Time
 REQUISITION ID: RBS42-001
 CITIZENSHIP: U.S. Citizen Required
 CLEARANCE REQUIRED: Top Secret/SCI
 TRAVEL REQUIREMENTS: Up to 10%

ðŸš€ Join Redbeard Solutions â€“ Where Innovation Meets Mission

At Redbeard Solutions, we connect forward-thinking developers with mission-critical national security initiatives. We are proud to support the Joint Warfare Analysis Center (JWAC) in Dahlgren, VA, in providing advanced engineering and scientific analysis that shapes strategic military decisions.

Weâ€™re looking for a Senior DevOps Full Stack Developer to help design and secure next-generation applications. Youâ€™ll play a key role in building and optimizing secure DevSecOps pipelines that power essential tools for U.S. military leaders and defense planners.

ðŸ”§ Key Responsibilities:
Design, implement, and maintain secure DevSecOps pipelines
Integrate security at every stage of the software development lifecycle
Use tools like Jenkins, GitLab CI/CD, or CircleCI for continuous integration/deployment
Script automation using Python, Bash, or PowerShell
Work with Docker/Kubernetes for containerization
Promote secure coding practices, vulnerability scanning, and compliance
Collaborate across teams and support cloud-native and serverless environments

âœ… Required Qualifications:
3+ years in software development with DevSecOps focus
Experience with DevSecOps tools and scripting languages
Hands-on with containers, cloud-native tools, and cross-functional environments
U.S. Citizenship and active Top Secret clearance with SCI eligibility
High school diploma or GED
Ability to obtain Security+ (IAT II) within 30 days of start

â­ Preferred Qualifications:
Agile/Scrum/Kanban development experience
Background in AI/ML, cloud compliance, or data science
Familiarity with HIPAA, PCI-DSS, GDPR, and other security frameworks
Bachelorâ€™s in CS, Engineering, or related field
DevSecOps certification (Foundation or Engineer)

ðŸ“§ To Apply:
 Send your resume directly to: aayushd@redbeardsol.com

This is your opportunity to drive transformation in a high-impact environment while advancing your DevOps career.

Redbeard Solutions is proud to be an equal opportunity employer. We value diversity and are committed to fostering an inclusive workplace for all.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4259553670,https://www.linkedin.com/jobs/view/4259553670,Pending,Unknown,2025-07-10 10:36:26.161517,Required experience is high,"
About the job
Title: Senior DevOps Engineer
Location: Englewood, CO
Duration: 9-month contract, potential extensions
Schedule: Hybrid- 4 days onsite/week
Pay Rate: 65-75/hr
 Required Skills & Experience
10+ years of DevOps Experience
Python and/or Ansible
Kubernetes
Docker
Jenkins
 Nice to Have Skills & Experience
Background in telecom/cable
VMware
Grafana
MLOps
SRE/Infrastructure 
 Job Description
As a DevOps Engineer IV in Platform as a Service for on-premises cloud, you will be responsible for ensuring the reliability, availability, and scalability of our PaaS infrastructure, IaaS platforms, cloud management platform, CI/CD pipeline, automation, and tooling. You will work closely with our development, operations, and security teams to design, implement, and maintain a highly available and secure PaaS platform and managed services systems. Your primary focus will be on design, developing, and maintaining the service catalog, CI/CD pipelines, and automation required for our PaaS offerings, managed services tools, ensuring high availability, reliability, and performance of our services. You will be responsible for ensuring that our PaaS platforms, cloud management platform, and other managed services meets the needs of our internal and external stakeholders.
Design, develop, implement, and maintain Grafana Data Reporting and Visualization with specific familiarity working with VMWare VROPS data collection and presentation.
Automate deployment, monitoring, and management of PaaS and IaaS services.
Works with developers, testers, and deployment teams to create software deployment plans.

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4260752247,https://www.linkedin.com/jobs/view/4260752247,Pending,Unknown,2025-07-10 10:36:42.900681,Required experience is high,"
About the job
Job Title: DevOps Engineer
Experience: 8â€“10 Years
Location: Chicago, IL

Job Description:
We are seeking a Senior DevOps Engineer responsible for the analysis, design, and development of scalable, secure software solutions across the full software development lifecycleâ€”from conception to deployment. The ideal candidate will be proficient in both front-end and back-end technologies, with a strong emphasis on Java, Spring Boot, Azure (AKS), and JUnit/Gradle. You will be a key contributor to a cross-functional Agile team, driving innovation and automation in a cloud-native environment.

Key Responsibilities:
Design, develop, and maintain scalable microservices and cloud-native applications.
Implement CI/CD pipelines and automate infrastructure using DevOps best practices.
Collaborate with developers, QA, and operations teams to ensure seamless deployment and monitoring.
Optimize system performance, reliability, and security across environments.
Troubleshoot and resolve issues in development, test, and production environments.

Required Skillsets:
Programming Languages: Java (1.8, 21), Python, Shell scripting
Frameworks & Tools: Spring Boot, Spring Batch, Spring Integration, JUnit, Gradle, Maven
Cloud & DevOps: Azure, AKS, PCF, Docker, GitHub, CI/CD pipelines
Databases: Oracle, SQL, Cosmos DB, MarkLogic, NoSQL
Big Data & Messaging: Azure DataBricks, ADF, Kafka
Operating Systems: Unix/Linux
IDEs & Platforms: IntelliJ, Eclipse
Architecture: Microservices design and implementation

Preferred Qualifications:
Experience with Agile/Scrum methodologies
Strong problem-solving and communication skills
Ability to work independently and in a team-oriented environment

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4251535942,https://www.linkedin.com/jobs/view/4251535942,Pending,Unknown,2025-07-10 10:36:45.317078,Asking for Security clearance,"
About the job
Akkodis is seeking a Devops Engineer for a Contract job with a client in Dearborn, MI ""The ideal candidate should have experience as a Devops Engineer with GCP

Rate Range: $58/hour to $60/hour; The rate may be negotiable based on experience, education, geographic location, and other factors.

Devops Enginner Job responsibilities include:
Build and maintain CI/CD pipelines using tools like Jenkins, 
Manage infrastructure as code to provision, configure, and scale cloud environments (AWS/GCP/Azure) and container platforms (Docker, Kubernetes) 
Implement monitoring, logging, and performance optimization, setting up tools 
Troubleshoot production issues, and continuously improve processes

Desired Qualifications:
5+ Years of experience as a Devops Engineer
5+ Years of experience Distributed Systems & Cloud Expertise.
GCP Cloud Experience 
Bachelorâ€™s degree in Computer Science, Business Information Systems, or a related IT discipline.

If you are interested in this role, then please click APPLY NOW. For other opportunities available at Akkodis, or any questions, feel free to contact me at 610-227-6372 or kshitij.Ahlawat@akkodisgroup.com



Equal Opportunity Employer/Veterans/Disabled

Benefit offerings available for our associates include medical, dental, vision, life insurance, short-term disability, additional voluntary benefits, an EAP program, commuter benefits, and a 401K plan. Our benefit offerings provide employees the flexibility to choose the type of coverage that meets their individual needs. In addition, our associates may be eligible for paid leave including Paid Sick Leave or any other paid leave required by Federal, State, or local law, as well as Holiday pay where applicable. Disclaimer: These benefit offerings do not apply to client-recruited jobs and jobs that are direct hires to a client.

To read our Candidate Privacy Information Statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy.

The Company will consider qualified applicants with arrest and conviction records in accordance with federal, state, and local laws and/or security clearance requirements, including, as applicable:
Â· The California Fair Chance Act
Â· Los Angeles City Fair Chance Ordinance
Â· Los Angeles County Fair Chance Ordinance for Employers

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4249058234,https://www.linkedin.com/jobs/view/4249058234,Pending,Unknown,2025-07-10 10:36:49.846134,Asking for Security clearance,"
About the job
ClearanceJobs is assisting their partner, a small business and minority-owned engineering firm in Maryland, leading the IT industry with proven performance and innovative design, development, and integration services, has an immediate opening for an experienced Sr. Systems Engineer. The right candidate must have senior-level Java Developer experience to support critical national security initiatives!

An active TS/SCI w/FSP security is a MUST to apply!

Terms: Fulltime/Direct Hire
Location: Onsite (Annapolis Junction, MD)
Salary: $200k+ (will fluctuate pending experience)

Qualifications:
10+ years of experience as in programs and contracts of similar scope, type, and complexity is required.
Bachelorâ€™s degree in a technical discipline from an accredited college or university is required.
Scripting: Bash or Python
Configuration: Ansible/Puppet/Salt
Containerization: Docker or Kubernetes

Nice to have:
AWS Certified Cloud Practitioner Certification
Certified Kubernetes Administrator
DoD 8570 IAT certification

What youâ€™ll do:
Analyzes userâ€™s requirements, develop operations documents.
Deliver high-level system architectures to develop system requirements specifications.
Analyze system requirements and leads design and development activities.
Guide users in formulating requirements.
Advises on alternative approaches and conducts feasibility studies.
Provides technical leadership for the integration of requirements, design, and technology.
Incorporates new plans, designs, and systems into ongoing operations.
Develops technical documentation.
Develops system Architecture and system design documentation.
Guides system development and implementation planning through assessment or preparation of system engineering management plans and system integration and test plans.
Interacts with the Government regarding Systems Engineering technical considerations and for associated problems, issues or conflicts.
Ultimate responsibility for the technical integrity of work performed and deliverables associated with the Systems Engineering area of responsibility.
Communicates with other program personnel, government overseers, and senior executives.

More about the role:
Provide optimization and automation across multiple platforms and applications.
Drive consistency for deployment and build processes.
Utilize modern tools like Jenkins, Docker, Kubernetes, Amazon Web Services, Ansible, Terraform, Python, Linux, and much more.
You have worked with teams before on large and demonstrable projects (preferably built on with AWS and familiarity with scripting languages such as Bash, Ruby, Python).
Familiarity with containerization tools like Docker and Kubernetes.
You understand how to set up a CI/CD pipeline.
You understand Agile software development and DevOps practices and can work closely with your peers other mid-to large size teams.
Focus on understanding customers' needs and translating those needs from product specifications into functional, production ready code and infrastructure.
The following skills are required: AWS (S3, VPCs & Networking, EC2, ECS/EKS). Containerization (Docker, k8s, Registries). IaC (Terraform/Cloud Formation). CI/CD (Jenkins/ GitHub Actions).
You are self-driven, self-learner willing to share knowledge and participate actively in a team environment.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4241031476,https://www.linkedin.com/jobs/view/4241031476,Pending,Unknown,2025-07-10 10:36:58.767781,Asking for Security clearance,"
About the job
Job Title: DevOps Engineer
Location: Dallas, TX (Need GC and Citizen profiles)

Job Summary:
DevOps Engineer to design, implement, and maintain CI/CD pipelines, cloud infrastructure, automation, and monitoring solutions. The ideal candidate will have expertise in Azure DevOps with strong knowledge of Infrastructure as Code (IaC), Kubernetes, automation, and security best practices.
Key Responsibilities:
1. CI/CD Pipeline Automation
â€¢ Design and maintain CI/CD pipelines using Azure DevOps
â€¢ Automate code deployment, testing, and rollback strategies for microservices and APIs.
â€¢ Enable blue-green, canary, and rolling deployments to improve software release stability.
2. Cloud Infrastructure & Automation
â€¢ Build and manage cloud environments on Azure.
â€¢ Implement Infrastructure as Code (IaC) using Terraform, Bicep.
â€¢ Optimize cloud costs through auto-scaling, reserved instances, and FinOps best practices.
3. Containerization & Kubernetes
â€¢ Deploy and manage Kubernetes (AKS) clusters.
â€¢ Implement containerized applications using Docker and orchestration tools.
4. Security & Compliance
â€¢ Implement DevSecOps best practices, including RBAC, IAM, Secrets Management.
â€¢ Secure CI/CD pipelines with OWASP security checks, vulnerability scanning.

5. Monitoring & Incident Management
â€¢ Set up observability tools (Grafana, OpenTelemetry, Azure Monitor).
â€¢ Define alerting and logging strategies for proactive issue detection.
â€¢ Implement automated incident response using AI-driven monitoring solutions.
6. Collaboration & Process Improvement
â€¢ Work closely with development, security, and operations teams to improve DevOps workflows.
â€¢ Advocate for best practices in automation, SRE, and site reliability engineering.
â€¢ Provide technical mentorship and knowledge-sharing sessions.
Technical Skills
â€¢ CI/CD Tools: Azure DevOps, Jenkins, GitHub Actions, GitLab CI/CD.
â€¢ Cloud Platforms: Azure
â€¢ Infrastructure as Code (IaC): Terraform
â€¢ Kubernetes & Containers: Docker, AKS
â€¢ Monitoring & Logging: Prometheus,Grafana, OpenTelemetry, Azure Monitoring.
â€¢ Security & Compliance: DevSecOps, IAM, RBAC.
â€¢ Scripting & Automation: Python, Bash, PowerShell.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4255917020,https://www.linkedin.com/jobs/view/4255917020,Pending,Unknown,2025-07-10 10:37:06.243492,Required experience is high,"
About the job
DevOps Engineer - Strong Automation

SALARY: $150k - $165lk plus 15% bonus

LOCATION: Dallas, TX
3 DAYS ONSITE â€“ HYBRID

Looking for a Devops engineer with strong automation background. CI/CD, GitHub, Jenkins groovy k8s Kubernetes terraform Linux administration. Flexibility to be on call from 5p â€“ 7a for 3 months per year

Responsibilities:

To perform this job successfully, an individual must be able to perform each primary duty satisfactorily.

Identify, analyze, and automate repetitive or manual tasks across infrastructure and deployments.

Design, implement, and manage CI/CD pipelines to support agile development and rapid releases.

Script solutions using Bash, Python, or similar tools to streamline DevOps processes.

Advocate for and enforce DevOps and security best practices in infrastructure and deployment processes.

Collaborate with development teams to identify opportunities for automation and implement solutions to improve quality and efficiency.

Qualifications:

Technical Skills:

Proven experience as a DevOps Engineer with a strong automation background

Deep knowledge of CI/CD tools (e.g., GitHub Actions, GitHub, Jenkins - Groovy, K8s, helm, etc.).

Strong hands-on experience with Terraform for cloud infrastructure provisioning.

Proficiency in Linux administration and shell scripting (Bash, Python, or similar).

Solid understanding of AWS services (e.g., EC2, S3, RDS, Lambda, CloudWatch, IAM, etc.).

Strong understanding of software development methodologies, testing concepts, and Agile development practices, including Scrum and Kanban

Education and/or Experience:
Bachelor's degree in a related area
7-10 years of related experience
Minimum 5 years experience on scripting/devOps Automation
Minimum 3 years experience working with AWS services
Minimum 3 supporting and maintaining 

Certificates or Licenses:
Cloud Certification a plus

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4262158969,https://www.linkedin.com/jobs/view/4262158969,Pending,Unknown,2025-07-10 10:37:35.381057,Required experience is high,"
About the job
Sr. DevOps Engineer (Azure, GitHub Action, Ansible)
Fully Remote: Must be in EST or CST time zone
6-month contract-to-hire

Optomi, in partnership with our premier client in the insurance industry, is seeking an experienced DevOps Engineer to join their team for a remote role! The DevOps Engineer will support multiple product and development teams in automating infrastructure, streamlining CI/CD pipelines, managing deployments, and ensuring production readiness. The ideal candidate will bring deep expertise in Azure, Terraform, Ansible, and GitHub Actions, along with strong scripting and automation skills. You will collaborate with product managers, architects, developers, and stakeholders in a fast-paced agile environment to deliver scalable and resilient cloud infrastructure solutions.

Key Responsibilities:
Design, build, and maintain cloud infrastructure using Azure services (Identity, Networking, Compute, Storage, Automation, Disaster Recovery).
Develop and maintain infrastructure-as-code using Terraform and Ansible.
Create, enhance, and support CI/CD pipelines using GitHub Actions and GitHub Enterprise.
Write scripts and automation tools using Python, Bash, and PowerShell.
Operate and scale container platforms; support containerization and migration efforts.
Manage high-availability, cloud-based server architectures and enforce deployment standards.
Implement monitoring and alerting strategies; define telemetry and observability baselines.
Partner with cross-functional teams to troubleshoot and resolve production issues.
Apply DevOps and Agile best practices for continuous integration and delivery.
Document infrastructure components, automation tools, and deployment procedures.
Provide mentoring and technical guidance to junior DevOps engineers.

Required Qualifications:
7+ years of IT experience, with at least 2+ years in DevOps automation/integration.
Proven experience with:
Microsoft Azure (IaaS, PaaS, Networking, Identity, and Resource Management)
Terraform (Infrastructure as Code)
Ansible (Configuration Management)
GitHub Actions and GitHub Enterprise
Scripting and automation expertise in Python, Bash, and PowerShell.
Strong Linux administration skills and understanding of system internals.
Familiarity with software development life cycles and release management.
Experience with monitoring tools and defining performance metrics and thresholds.
Strong understanding of networking concepts and protocols.
Comfortable working in multi-cloud or hybrid environments.
Excellent problem-solving, communication, and documentation skills.

Preferred Qualifications:
Bachelorâ€™s degree in Computer Science or related field (or equivalent experience).
Relevant certifications (e.g., Azure Administrator, Terraform Associate, RHCE).
Experience in regulated or enterprise environments (e.g., insurance or finance).
Prior experience in Agile environments and test-driven development methodologies.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4246274368,https://www.linkedin.com/jobs/view/4246274368,Pending,Unknown,2025-07-10 10:37:38.256735,Asking for Security clearance,"
About the job
The Network DevOps Engineer III will support the C2BMC program and will be responsible for developing, implementing, and managing our CI/CD pipeline tools, including GitLab, Nexus, and Python. The candidate will also support the configuration management processes, releases, and versioning of managed configurations, and create automated pipelines to deploy new configurations into lab environments.

Location
Colorado Springs, CO
This role must be able to travel to alternate work locations in the future.

The Work Youâ€™ll Do
As a DevOps Engineer, you will play a critical role in developing, implementing, and managing our CI/CD pipeline tools, including GitLab, Nexus, and Python. You will also be responsible for executing configuration management processes, releases, and versioning of managed configurations, and creating automated pipelines to deploy new configurations into lab environments.
Apply principles, theories, and concepts to provide imaginative and thorough solutions to a wide range of difficult problems
Work under general direction, with results reviewed upon completion for adequacy in meeting objectives
Support the C2BMC Program as a DevOps Engineer
Administer GitLab, ensure proper documentation, and reporting
Execute data management utilizing existing program tools, including documentation, training, and reporting
Assist with release management as required
Develop, deploy, and maintain a CI/CD pipeline for the NES ART
Create, maintain, and present KPIs and other metrics related to the CI/CD pipeline and configuration management areas
Create and maintain a test pipeline through multiple labs
Perform other tasks as required by the program

What Youâ€™ll Bring
Bachelorâ€™s degree with 0-2 yearsâ€™ experience, or equivalent
Active Secret clearance required to start.
Active IAT Level II (DOD 8570) Certification required to start (Security + or equivalent)
Effective communication and writing skills, with proficiency in Microsoft Office products (especially PowerPoint and Visio)
Experience with GitLab and GitLab concepts (such as branching, merging, etc.)
Detail-oriented with the ability to follow processes
Proficient in Python, Bash and PowerShell

Preferred
Active Top-Secret Clearance desired. Secret is required to start.
Experience working with Jira and Confluence
Basic knowledge of Network Engineering
Experience creating and updating processes and procedures
Experience withing with automation tools such as Ansible.
Developing KPIs and tracking metrics, with experience creating reports
Experience working in a DoD environment
Prior experience with C2BMC


Salary Range: $100,000 â€“ $148,000


Our Approach
At SecureStrux, we are committed to core values that guide the way we work with one another and our clients. As a team member, you will Create Team Synergy, Drive Continuous Innovation, Deliver with Integrity, and have the Freedom to Own it. Our thriving company culture supports our employees as they seek to grow with us!

What We Offer
Between our virtual environment where you can evaluate recent technologies and enhance your skills, and a generous annual professional development stipend, you will join a team that enjoys working on leading-edge technologies for world-class clients. We offer a robust total compensation package that includes comprehensive health benefits to support you and your family, flexible time off, continuing education allowance, a donation allowance for charitable causes, and a matched 401k.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4253260065,https://www.linkedin.com/jobs/view/4253260065,Pending,Unknown,2025-07-10 10:37:40.858657,Required experience is high,"
About the job
About Encore Talent Solutions:
Encore Talent Solutions is a trusted professional services firm dedicated to helping organizations achieve their goals by providing exceptional talent solutions. We partner closely with our clients to understand their unique culture and operational needs, delivering proactive support during times of growth, transition, and change. Our mission is to connect top talent with meaningful opportunities to drive business success.

KEY RESPONSIBILITIES
Design, implement, and manage infrastructure using Terraform (Infrastructure as Code)
Build and maintain robust CI/CD pipelines to enable fast, reliable, and secure software deployments
Automate operational tasks and processes using JavaScript or C#
Collaborate across teams to design scalable and resilient systems
Monitor and troubleshoot production environments, ensuring high availability and performance
Support and enhance infrastructure build processes in AWS

Must-Have Qualifications:
7+ years of overall IT experience,
Proven AWS build experience, including provisioning and managing cloud resources
Hands-on experience with Terraform for infrastructure automation
Solid experience developing and supporting CI/CD pipelines (e.g., Jenkins, GitLab CI, GitHub Actions, Azure DevOps)
Proficient in a scripting language: 5 years of experience with python, golang, bash/powershell, javascript. 
Strong understanding of DevOps best practices and cloud-native architecture
Knowledge of monitoring and observability tools (e.g., Prometheus, Grafana, CloudWatch)

Nice to Have:
Experience with Aurora or PostgreSQL database platforms
Familiarity with containerization/orchestration (Docker, Kubernetes)
Experience building distributed applications and microservices
Experience with event-driven architecture
Experience working in Agile/Scrum environments

Encore Talent Solutions is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4245195957,https://www.linkedin.com/jobs/view/4245195957,Pending,Unknown,2025-07-10 10:38:10.540463,Asking for Security clearance,"
About the job
Summary
The Cloud DevOps Engineer primary responsibilities will be defining and provisioning infrastructure resources using code, enabling automated and repeatable deployments. This eliminates manual configuration, reduces errors, and ensures consistency across environments. Cloud DevOps Engineers must be proficient in tools like Terraform, CloudFormation, GCP, and Azure Resource Manager to define infrastructure components such as virtual machines, networks, storage, and databases.
Furthermore, they are responsible for automating various aspects of the software development lifecycle, including build processes, testing, and deployments. Automation streamlines workflows, accelerates delivery times, and improves overall efficiency. This often involves scripting using languages like Python, Bash, or PowerShell, and integrating with automation platforms such as Ansible, Chef, or Puppet. A Cloud DevOps Engineer should have a deep understanding of automation principles and the ability to design and implement automated solutions that meet specific business requirements. By leveraging automation, Cloud DevOps Engineers enable organizations to achieve greater agility, scalability, and resilience in their cloud environments. 

Need to have hands-on experience across multi-cloud environments ( Azure, GCP) and extensive expertise in DevOps automation, CI/CD pipeline management, infrastructure-as-code (IaC), containerization, and cloud security.
  The ideal candidate will be responsible for building, managing, and optimizing DevOps pipelines, driving innovation, and introducing new solutions to streamline our cloud and DevOps processes. This role requires a deep understanding of cloud architecture, automation, microservices, and security best practices to support scalable and resilient cloud-based applications.

Essential Functions 
Automated Provisioning - CI/CD automates testing, reducing manual checks. This ensures quicker feedback. Develop and maintain a comprehensive security architecture covering on-premises, cloud, and hybrid environments.
Consistency -ensuring consistent environments across development stages. This reduces errors during deployment. Design security solutions that align with business objectives while mitigating risk.
Real-time Monitoring - Cloud monitoring provides real-time insights into system performance. Version Control - Infrastructure configurations are version-controlled. This enables tracking and reverting changes easily. Ensure Zero Trust principles, network segmentation, and security best practices are enforced across the enterprise.
Proactive Issue Detection - Identify and resolve potential issues. This can be done before they impact users. Architect cloud security strategies, leveraging best practices for Azure and GCP.
Centralized Logging - Centralized logging helps aggregate and analyze logs. Logs are collected from different sources. Lead security investigations, conduct root cause analysis, and document incident response actions.
Configuration Management - Scripting tools manage configurations. They ensure consistency. Provide threat intelligence and recommend proactive security measures to mitigate risk.
Infrastructure Orchestration - Orchestration tools automate infrastructure setup. This streamlines processes. Implement and maintain cloud security controls in Azure and GCP.
Security Policies - Implementing security policies to protect data and systems. Optimize cloud security solutions for web and network protection.
Access Control - Managing access control to limit unauthorized access. This is essential for security. Ensure Active Directory (AD) and IAM policies align with best practices.
Cross-functional Teams - Working with development, operations, and security teams. 
Documentation - Creating and maintaining clear documentation for processes. Oversee the continuous best practice is leveraged for data classification policies and enforce data protection controls.

Additional Functions 
Cloud Infrastructure & Automation:
Design, build, and manage secure, scalable, and high-availability cloud environments across AWS, Azure, and GCP.
Develop and maintain Infrastructure-as-Code (IaC) solutions using Terraform, CloudFormation, Pulumi, and Ansible.
Implement multi-cloud strategies, hybrid cloud deployments, and cloud networking solutions.
Optimize cloud costs through monitoring, auto-scaling, and resource provisioning techniques.
DevOps & CI/CD Pipeline Management:
Architect, implement, and maintain CI/CD pipelines using tools like Jenkins, GitHub Actions, GitLab CI/CD, Azure DevOps, and AWS CodePipeline.
Automate build, test, deployment, and rollback processes for applications and infrastructure.
Ensure secure DevOps practices, including secrets management, policy-as-code, and automated compliance.
Integrate observability, logging, and monitoring solutions within the pipeline (e.g., ELK, Prometheus, Grafana, Datadog, New Relic).
Containerization & Kubernetes:
Deploy and manage containerized applications using Docker and Kubernetes (EKS, AKS, GKE).
Implement Kubernetes security best practices, monitoring, and autoscaling.
Automate Kubernetes deployments using Helm, Kustomize, and GitOps tools like ArgoCD and Flux.
Security & Compliance in Cloud and DevOps:
Ensure cloud security best practices by implementing IAM, RBAC, security groups, and encryption.
Deploy security tools such as AWS Security Hub, Azure Security Center, Prisma Cloud, and GuardDuty.
Manage identity and access controls for DevOps tools and cloud services.
Conduct regular security assessments and vulnerability management across cloud workloads.
Observability, Performance, & Reliability Engineering:
Implement SRE (Site Reliability Engineering) principles to improve system reliability and incident response.
Develop and integrate logging and monitoring solutions using Prometheus, Grafana, Datadog, Splunk, or ELK.
Build automated alerting and response mechanisms for cloud and DevOps environments.
Implement chaos engineering to improve system resilience and fault tolerance.
Innovation & Continuous Improvement:
Continuously evaluate new DevOps and cloud technologies to improve efficiency and scalability.
Automate repetitive tasks and enhance self-service capabilities for development teams.
Participate in architecture discussions, design reviews, and proof-of-concept (PoC) implementations.
Lead DevOps culture adoption across teams by driving best practices and training initiatives. Conduct threat modeling, risk assessments, and security reviews for applications, infrastructure, and networks.

Qualifications 
7-10 years of hands-on experience in Cloud & DevOps Engineering roles.
Expertise in multi-cloud environments (Azure and GCP).
Deep understanding of DevOps methodologies, CI/CD pipeline design, and automation.
Strong experience with Terraform, Ansible, CloudFormation, and Kubernetes.
Proficiency in Jenkins, GitHub Actions, GitLab CI/CD, and Azure DevOps.
Hands-on experience with Docker, Kubernetes (EKS, AKS, GKE), Helm, and GitOps (ArgoCD, Flux).
Knowledge of cloud security, IAM, RBAC, and compliance frameworks (SOC2, NIST, ISO 27001).
Proficiency in scripting and automation using Python, Bash, PowerShell, or Go.
Experience with observability tools like Prometheus, Grafana, ELK, and Datadog. 


Working Conditions & Physical Demands 
This position requires in person office presence at least 4x a week.

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4255193545,https://www.linkedin.com/jobs/view/4255193545,Pending,Unknown,2025-07-10 10:38:21.070624,Found a Bad Word in About Job,"
About the job
SENIOR DEVOPS ENGINEER

WEâ€™RE LOOKING FOR AN EXPERIENCED SENIOR DEVOPS ENGINEER TO BECOME AN INSTRUMENTAL PART OF OUR NEXT PHASE OF GROWTH!

NO H1B or C2C CANDIDATES CAN BE CONSIDERED FOR THIS ROLE - ONLY US CITIZENS AND GREEN CARD HOLDERS ELIGIBLE.

Senior DevOps Engineer
Minneapolis, MA or Austin, TX (relocation package offered)
$130,000 - $170,000 + benefits!
Benefits - Full health cover (medical/dental/vision) & paid PTO!
To apply please email / paolo.melacrinis@searchability.com

WHO ARE WE?
We are a Austin-based Robotics startup in a very excited period of growth! You will join a world class engineering team with experience from some of the biggest names in tech. You will be responsible for building and maintaining our custom-built platform, a cloud native and self-hosted application (hosted in AWS and on-prem), and assisting the deployment automation for cloud and on-prem environments. The Senior DevOps Engineer manages Linux and Docker containers, writes CI/CD scripts, maintains our Postgres database and cloud systems to ensure that our platform runs smoothly for our customers and internal teams.

WHAT WILL YOU BE DOING?
Ensure high availability and reliability of the AO cloud platform by proactively monitoring system performance, implementing redundancy, and responding promptly to incidents.
Maintain and collaboratively lead in the overall application lifecycle, the evaluation and selection of cloud utilities, standards, interface protocols, and software components and tools.
Engage with the technical team, ensuring overall success and contribute to technical requirements documents and problem solutions.
Design, build, and maintain a stable and efficient infrastructure to optimize service delivery across production, QA, and development environments throughout the development lifecycle.
Monitor, troubleshoot, maintain, and continuously improve build, packaging, and deployment processes.
Implement automated infrastructure capabilities like backups, monitoring, and security tools.

WE NEED YOU TO HAVEâ€¦.
BS Degree in Computer Science or equivalent experience.
4-7 years of related experience.
Experience managing and deploying applications in AWS, with a strong understanding of cloud-native services, networking, and security best practices.
Demonstrated expertise in Linux system administration, including configuration, troubleshooting, and performance tuning.
Proven experience in administering and optimizing databases, including PostgreSQL, ensuring data integrity, performance, and scalability.
Working knowledge of Infrastructure as Code (IaC) tools such as Terraform, with the ability to automate and manage cloud infrastructure efficiently.
Proficiency in Python, Bash, Terraform, or a strong software development background.
Experience with CI/CD pipeline tools with a focus on automating build, test, and deployment processes.
Strong experience with Docker, including building, managing, and optimizing container images, as well as deploying and orchestrating containers in development and production environments.

IT WOULD BE GREAT IF YOU HAVE EXPERIENCE IN ONE OF THE FOLLOWING.....
Experience working with geospatial data and tools.
Experience with Windows and macOS administration

TO BE CONSIDEREDâ€¦.
Please either apply by clicking online or emailing me directly to paolo.melacrinis@searchability.com. I can make myself available outside of normal working hours to suit from 7am until 10pm. If unavailable, please leave a message and either myself or one of my colleagues will respond. By applying for this role, you give express consent for us to process & submit (subject to required skills) your application to our client in conjunction with this vacancy only. Also feel free to follow me on Twitter @SearchablePM or connect with me on LinkedIn, just search Paolo Melacrinis in Google! I look forward to hearing from you.

DevOps, AWS, Docker, Python, Bash, CI/CD, Terraform, Linux, Robotics, Autonomous, AUV, Subsea

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4258494364,https://www.linkedin.com/jobs/view/4258494364,Pending,Unknown,2025-07-10 10:39:07.663089,Asking for Security clearance,"
About the job
Job Title: Senior Kubernetes Engineer
Location: Dallas, TX

We're looking for a highly skilled Senior Kubernetes Engineer to join our team. You'll design, implement, and maintain robust Kubernetes solutions, focusing on scalability, security, and cost optimization. This role involves supporting application teams and ensuring seamless operation of their services on our Kubernetes infrastructure.
Key Responsibilities:
Manage EKS/AKS clusters, debugging complex application and cluster issues.
Implement comprehensive monitoring with Prometheus, AlertManager, Grafana, and Datadog (including APM).
Implement and manage centralized logging with the EFK (Elasticsearch, Fluentd, Kibana) stack.
Drive GitOps workflows using ArgoCD and deploy applications with Helm.
Configure Nginx Ingress (or similar), manage CSI storage (EFS/EBS), and implement Cilium network policies.
Enhance cluster security using OPA Gatekeeper and manage secrets with External Secrets Provider/Sealed Secrets and cert-manager.
Optimize costs and implement autoscaling with Karpenter, Cluster Autoscaler, HPA, VPA, and KEDA.
Provide expert support for application onboarding and troubleshoot network connectivity issues.
Create and maintain high-quality technical documentation.
Qualifications:
10+ years in a Kubernetes, DevOps, or SRE role, with extensive EKS/AKS experience.
Proven ability to debug complex issues, including network connectivity.
Hands-on experience with listed monitoring, CI/CD, security, and autoscaling tools.
Strong understanding of Kubernetes networking, storage, and security best practices.
Excellent communication skills for technical discussions and documentation.
Bonus Points:
Kubernetes certifications (CKA, CKAD, CKS).
Knowledge of Rafay Platform will be plus

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4263737553,https://www.linkedin.com/jobs/view/4263737553,Pending,Unknown,2025-07-10 10:39:16.630886,Found a Bad Word in About Job,"
About the job
Direct Hire or 6 Month CTH $70.00 per hr W-2 or Conversion Salary $110K - $115K
Hybrid Role 3 days onsite Downtown Cincinnati, OH 45202 - Green Card or US Citizen ONLY


The Cloud Administrator position is to provide technical support for the management and maintenance of a Hybrid infrastructure. Provide support for Hybrid Cloud/On premise environments. The position also provides support for all servers, blade centers, storage, and installed desktop software, including Operating Systems. Facilitate escalation of problems, issues, and requests to other IT Department roles as necessary. Participate in and contribute to various IT Department projects and initiatives. Maintain positive contact with, staff, and clients and observe confidentiality in all client matters.
 
Responsibilities
Participate in planning and execution of the current cloud / hybrid infrastructure projects 
Participate in monitoring of all computer infrastructure systems to ensure maximum uptime
Participate in the ongoing cloud migration initiatives and projects
Administration and Maintenance of: Microsoft 365 and Azure Cloud including Entra, Exchange Online, SharePoint Online, Teams and all other systems; Microsoft Endpoint Manager (Intune) including MDM, MAM, Conditional Access Policies and Compliance Policies; Microsoft Defender for Endpoint & Defender for Office 365 to ensure Security & Privacy compliance; Provision and Manage access to third party cloud applications though Azure SSO; Securing End User Computing systems like Laptops, Desktops & Mobile devices; Endpoint Security systems like Antivirus/Anti Malware, Firewall, DLP etc.; other cloud systems
Troubleshoot problems associated with any server hardware, software, peripherals, and network connectivity; determine necessary repair, modifications, and testing of such systems
Escalate problems to proper channels to ensure corrective action is taken to achieve maximum results in minimal time
Continually seek methods to maintain and modify appropriate security measures as needed for the firm
Ensure physical inventory and configuration documentation of servers, storage, network devices, and related equipment is accurate and continually updated
Create and modify VB and PowerShell scripts
Administer third party products that integrate and/or are part of the overall toolset used in areas of responsibility.
Participate in ongoing monthly patch maintenance and server upgrades
Demonstrate teamwork by engaging in discussions with co-workers and promote a team environment where co-workers can collaborate in order to provide effective problem-resolution 
Demonstrate initiative by contributing new ideas, being self-motivated, and obtaining/maintaining a working knowledge of relevant areas
Stay current on technology trends and make recommendations appropriate to the firmâ€™s environment and business requirements
Demonstrate organizational skills and effective use of time. Plan and prioritizing daily work and managing time to ensure work is timely and efficiently completed
Maintain updated documentation of all systems in written technical form and visual media
Demonstrate flexibility and a willingness to adjust to changes in job requirements and scheduling
Be present and prepared for work as scheduled, use time off policies appropriately
Utilize firm procedures and resources appropriately to ensure efficient delivery of work product
Perform other duties, responsibilities and/or special projects as assigned


Requirements
A Bachelorâ€™s degree in a related technical field preferred or an Associateâ€™s degree with 5 years of related technical positions. Preferential consideration to candidates with Microsoft M365 experience and certifications
Experience with Hybrid Cloud/On-premise environments
Experience with migrating on-premise environments to the cloud
Experience with Microsoft 365 & Azure Cloud including Azure, Exchange Online, SharePoint Online, Single Sign On, Teams and all other systems
Experience with Microsoft Endpoint Manager (Intune) including MDM, MAM, Conditional Access Policies and Compliance Policies
Experience with Microsoft Defender for Endpoint & Defender for Office 365 to ensure Security & Privacy compliance
Knowledge of Endpoint Security systems like Antivirus/Anti Malware, Firewall, DLP etc.
In-depth knowledge of PowerShell 3.0 as well as common commands to support AD and Exchange
Scripting capabilities, including moderate level of VB Script
Understanding of Active Directory and Azure Active Directory (Entra ID) organization that include, but not limited to sites, OUs, containers, GPOs, policies and preferences
Represent IT Operations in Internal & External Compliance Audits
Create and maintain up-to-date documentation of IT SOPs, Policies & Standards
Experience with or the ability to achieve expert level knowledge of Microsoft virtualization technologies that include Hyper-V in a blade center environment
Experience utilizing standard toolsets to monitor and review infrastructure systems
Experience auditing and reviewing data to identify potential security risks
Experience supporting on-premise system infrastructure hardware and Windows Server operating systems and Microsoft Exchange
Strong Knowledge and experience with PKI and Certificate management
General knowledge and troubleshooting of DNS, DHCP, Static IPs, subnetting, APIPA 
Ability to effectively prioritize and execute plans in a high-pressure environment with minimal supervision
Ability to design and implement projects in conjunction with co-workers and/or outside contractors and vendors
Interpersonal skills necessary to communicate technical issues to a non-technical audience in a professional manner
Strong analytical and problem-solving skills
Strong customer service skills
Strong analytical skills
Ability to work in a fast-paced environment
Willingness to work extended hours, nights and weekends when necessary
Ability to travel to other offices when needed
Ability to transport, install, and maintain equipment, including computers, printers, and monitors. This requires occasional transportation of devices in excess of 50 lbs, and lying, stooping and bending to fully install various pieces of equipment
  CBTS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a protected veteran in accordance with applicable federal, state and local laws.

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4264603474,https://www.linkedin.com/jobs/view/4264603474,Pending,Unknown,2025-07-10 10:39:31.335694,Found a Bad Word in About Job,"
About the job
NO C2C- W2/1099 only

Job Title: Azure DevOPs Engineer with Copado - Hybrid
Location: New York, NY
USC/GC preffered


Job Description
We are looking for a skilled Azure DevOps Engineer with experience in Copado and Azure DevOps to support the secure and efficient delivery of software solutions in the utility and energy industry. This role is ideal for someone passionate about automation, infrastructure as code, and secure software delivery in a highly regulated environment. Youâ€™ll work closely with cross-functional teams to ensure the reliable and efficient deployment of applications that power modern energy infrastructure.

Key Responsibilities

CI/CD Pipeline Development & Management - Azure DevOps.

Design, implement, and maintain CI/CD pipelines using Azure DevOps.
Automate build, test, and deployment processes to support high-availability energy systems.
Collaborate with development, QA, and operations teams to streamline software delivery.
Mentor team members on DevOps and Agile best practices.
Infrastructure as Code (IaC) - Azure DevOps.

Manage infrastructure using tools like Terraform, Ansible, or CloudFormation or other IaC tools.
Support hybrid cloud and edge computing environments common in utility operations.
Ensure infrastructure is scalable, reproducible, and compliant with industry standards.
Agile Process & Project Management - Azure DevOps.

Define and manage Agile work items in Azure DevOps Boards (Epics, Features, User Stories, Tasks).
Collaborate with Product Owners and Scrum Masters to align sprint planning with business goals.
Create and maintain custom workflows and process templates.
Generate dashboards and reports to track team performance and project progress.
Security & Compliance (DevSecOps)

Integrate security practices into the DevOps pipeline to support a DevSecOps approach.
Qualifications

Bachelorâ€™s degree in Computer Science, Engineering, or a related field.
3+ years of experience in DevOps or related roles.
Hands-on experience with Azure DevOps, CI/CD tools, and scripting languages (e.g., PowerShell, Bash, Python).
Strong knowledge of IaC tools (Azure DevOps.).
Familiarity with Agile methodologies and project management tools.
Experience in the utility or energy industry is highly preferred.
Familiarity with regulatory compliance frameworks (e.g., NERC CIP, ISO 27001).


Best Regards,
Grace Abinezer
Recruitment Consultant | H3 Technologies, LLC 
ðŸ“ž (859) 287-0731
ðŸ“§ grace@h3-staffing.com
ðŸŒ www.h3-technologies.com SSN- 5406
71 Cavalier Blvd., Suite 208, Florence, KY - 41042
linkedin.com/in/grace-ebenezer-bb66a0251

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4264611548,https://www.linkedin.com/jobs/view/4264611548,Pending,Unknown,2025-07-10 10:39:33.698723,Asking for Security clearance,"
About the job
Role: AWS Engineer (Fulltime position)
Location: Charlotte, NC

Job Description:
Must have experience with similar platform engineering/management solutions:
Â·Building/optimizing Data LakeHouse with Open Table formats
Â·Kubernetes deployments/cluster administration
Â·Transitioning on-premise big data platforms to scalable cloud-based platforms like AWS
Â·Distributed Systems, Microservice architecture, and containers
Â·Cloud Streaming use cases in Big Data Ecosystems (e.g., EMR, EKS, Hadoop, Spark, Hudi, Kafka/Kinesis)

Must have hands-on experience with below tech stack:
Git Hib and Git Hub Actions
AWS â€“ IAM, API Gateway, Lambda, Step Functions, Lake Formation, Glue (Catalog, ETL, Crawler), Athena, S3 (Strong foundational concepts like object data store vs block data store, encryption/decryption, storage tiers etc)
Python
EKS & Kubernetes, Apache Hudi, Flink, PostgreSQL and SQL, RDS
Java background
Terraform Enterprise
Helpful tech stack experience would include:
Helm, Kafka and Kafka Schema Registry, AWS Services: CloudTrail, SNS, SQS, CloudWatch, Aurora, EMR, Redshift, Iceberg, Vault, AWS Secrets manager
 




Regards,
Bharath Kumar
bharath.k@themesoft.com

Found ""Clearance"" or ""Polygraph"". Skipping this job!
",Skipped,Not Available
4261924607,https://www.linkedin.com/jobs/view/4261924607,Pending,Unknown,2025-07-10 10:39:55.661930,Found a Bad Word in About Job,"
About the job
Role Overview:
Join a US-based deep tech innovator using a large satellite constellation to deliver real-time Earth observation. This is a hands-on Cloud Security role, ideal for someone with experience implementing frameworks like NIST 800-171 across cloud-native AWS environments.
You'll own security design and implementation during a key compliance phase, collaborating with DevOps and Engineering to build scalable, secure cloud solutions in Kubernetes.

Key Responsibilities:
Implement and maintain NIST SP 800-171 compliance across AWS environments
Build automated detection and response tools (CloudTrail, GuardDuty, CloudWatch)
Support secure architecture design and DevOps integration
Maintain the cloud security risk register and guide policy development
Contribute to DevOps sprints from a security-first perspective

Requirements:

3+ years' experience in cloud security (AWS, Kubernetes, containerized apps)
Proven track record implementing frameworks like NIST, ISO 27001, HIPAA, or CMMC
Scripting for automation (Python, Bash)
Familiarity with AWS security tools (Security Hub, WAF, SIEM, etc.)
US Citizenship required

Preferred Certifications:

AWS Certified Security, CISSP, CISM, CKS

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4264033635,https://www.linkedin.com/jobs/view/4264033635,Pending,Unknown,2025-07-10 10:40:27.849624,Required experience is high,"
About the job
Please find details for this position below:
Client: Banking/Financial Industry
Title: Principal DevOps Platform Engineer / Principal DevOps Engineer / Principal DevOps Architect
Location: Charlotte, NC / MN/ â€“ Hybrid Roles - 3 Days Hybrid role
Duration: 12+ Month (s) Contract, possibility to extend is not currently certain

Hiring Manager Notes:
Experience with larger organization and/or applications. Experience with the CICD tools.
DevOps position, but they want someone coming from a development background, but they wonâ€™t be a developer in this role, more of a mentorship position for the sr. engineers 
Development background, Java or .Net, doesn't really matter has long as they have been a developer.


Job Details:
Required Qualifications:
Client is seeking a Senior Engineer to be part of the Chief Technology Office (CTO) Platform Services team. Enterprise Pipeline (EPL) Platform Services team has key responsibilities for the Enterprise Pipeline across all applications in the Client's Enterprise.
Our strategy focuses on building a fully automated (build, deploy, and test), end-to-end pipeline that includes a community driven inner source model, establishes self-service onboarding capabilities, and integrates software development lifecycle and segregation of duties controls.
This Senior Engineer within EPL Platform services team will work across the EPL Product Owners, Engineering and applications teams to identify and implement solutions that advance our strategy. As a key engineering leader across EPL team, they will also collaborate with Client's Technology teams that use the EPL and key stakeholders to assess product gaps and ensure identification and delivery of product offerings that achieve desired business value and outcomes.

Skills:
Experience in full stack (Infrastructure Virtual Machines, Storage (NAS), Database, Application Languages Java, Python)
Experience in identifying and implementing Non-Functional Requirements
Experience in Observability tools like AppDynamics, Elastic, SPLUNK, Thousand Eyes, Grafana, Google Cloud Logging.
Experience in DevOps Tools like GITHUB, Jenkins, Artifactory, SonarQube, BlackDuck, Harness
Experience in Cloud Technologies like Azure, TKGI, Open Shift.
7+ years of Engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education.
3+ years of experience in two or more of the following tenets - Observability, Automation, Reliability, Resiliency, Scalability, Configuration Management & Actionable, Data Driven insights.
5+ years of experience in application & data architecture and solution design 3+ years of experience with secure DevOps and deployment automation to cloud environments.
5+ years of experience in application monitoring, performance management, analysis and troubleshooting skills.
5+ years of experience with standard industry standard performance tools and process.
3+ years of SRE experience in developing automated solutions for operational aspects in an organization.
2+ years of experience with Cloud technologies/Kubernetes/Containers

EEO:
Mindlance is an Equal Opportunity Employer and does not discriminate in employment on the basis of â€“ Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4263746011,https://www.linkedin.com/jobs/view/4263746011,Pending,Unknown,2025-07-10 10:40:37.858427,Found a Bad Word in About Job,"
About the job
City: Bristol, CT
Onsite/ Hybrid/ Remote: Onsite (4 days a week onsite)
Duration: 24 months
Rate Range: Up to$85/hr on W2 depending on experience (no C2C or 1099 or sub-contract)
Work Authorization: GC, USC, All valid EADs except OPT, CPT, H1B

Must Have:
Golang, Python, AWS, OpenTelemetry, Terraform or CloudFormation, Kubernetes, CI/CD


Responsibilities:
We are hiring for two Observability Engineer rolesâ€”one with a Development focus, and another with a DevOps focus. Both roles will play a pivotal part in building and scaling next-generation observability tools and infrastructure.

Design, build, and maintain observability pipelines using OpenTelemetry, Golang, and AWS.
Develop and manage CI/CD job templates to support automated testing for APIs and web applications.
Collaborate cross-functionally with tech leads, engineering managers, and QA stakeholders to gather requirements and prioritize initiatives.
Customize CI/CD processes, configure test environments, and validate service integrations.
Lead planning, estimation, and delivery tracking for engineering deliverables.
Maintain technical documentation and contribute to team-wide knowledge sharing.
Address technical debt and ensure ongoing stability and performance of observability infrastructure.
Adapt to new technologies and frameworks to meet evolving team and project needs.
Qualifications:
5+ years of software engineering experience with strong hands-on skills in Golang and/or Python.
Proven experience working in AWS production environments; AWS certifications are a plus.
Deep understanding of AWS Well-Architected Framework and cloud-native architecture.
Experience with Infrastructure as Code tools (Terraform or CloudFormation).
Familiarity with Kubernetes and modern orchestration tools.
Solid software development fundamentals, including unit/integration testing and CI/CD best practices.
Strong communication and collaboration skills; adaptable to fast-paced environments.
Bachelorâ€™s degree in a STEM field.
Preferred Qualifications:
Scripting experience (e.g., Bash, PowerShell).
Knowledge of observability tools such as Grafana, Datadog, or New Relic.
Exposure to front-end development with JavaScript, TypeScript, or React.
Experience with CI/CD platforms like GitLab CI/CD, GitHub Actions, Jenkins, or Spinnaker.

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4264612905,https://www.linkedin.com/jobs/view/4264612905,Pending,Unknown,2025-07-10 10:40:55.519539,Found a Bad Word in About Job,"
About the job
Hi Connections,
Looking for resources on below role, please share suitable resumes to ravindra@unicomtec.com

Position: Software Engineer (DevOps and Databricks experience) Only AWS Data Bricks No Azure data bricks
Location: Remote(Resources Across USA Can Apply)
only on W2 no C2C

************************Strictly No Corp to Corp Resumes will be considered*****************************
At least 4-5 years of experience in a DevOps or similar role.
Strong/Very good understanding of AWS services and cloud architecture.
Proficiency with Databricks (must) and terraform.
Strong hands-on experience with Terraform, AWS CDK, Python, YAML, and designing Databricks role-based permissions for database objects.
Understanding various Databricks computes, Unity Catalog, and network security.
Experience with CI/CD tools and infrastructure as cod

Qualifications
[Some qualifications you may want to include are Skills, Education, Experience, or Certifications.]
Example: Excellent verbal and written communication skills

Contains bad word ""No C2C"". Skipping this job!
",Skipped,Not Available
4263389373,https://www.linkedin.com/jobs/view/4263389373,Pending,Unknown,2025-07-10 10:41:15.466341,Required experience is high,"
About the job
***Hybrid, 3 days onsite, 2 days remote***
***We are unable to sponsor as this is a permanent full-time role***
A prestigious company is looking for an AWS Cloud Engineer. This role is focused on AWS, infrastructure as a code, Terraform, AWS, Kubernetes, Kafka, etc. 
Responsibilities:
Develop and maintain a high-quality library of deployable, tested, and documented automation design scripts, processes, and procedures.
Deliver development objectives and complex development tasks that will involve working with tools such as Docker, Kafka and container management systems.
Implement production changes during defined maintenance windows and support on call rotation.
Perform cloud computing and virtual environment build-outs, software installation, maintenance and support (i.e. Terraform)
Work on a dynamic and very broad range of cloud projects requiring on-the-project learning and use of various best in breed open-source projects
Manage customer expectations and go the extra mile (communication, documentation, testing) to meet customer needs and build long lasting partnerships.
Write and maintain documentation of relevant operating procedures and processes
Design, develop, and build the businessâ€™s cloud infrastructure architecture
Qualifications:
Bachelorâ€™s degree, preferably in a technical discipline (Computer Science, Mathematics, etc.), or equivalent combination of education and experience required
7+ years experience in IT systems installation, operations, administration, and maintenance of virtualized servers/cloud systems.
AWS Solutions Architect Associate Certification or higher strongly desired
Strong experience managing/building cloud infrastructure (storage, compute, networking, etc.) using automation.
Experience with architecting, implementing and maintaining highly available cloud environments for 24/7 availability.
Strong hands-on experience scripting/development skills in Python, Ruby, Go, Java, JavaScript, etc. in a corporate environment
Hands-on experience with: Terraform, Kubernetes, Jenkins, Kafka, GitHub, and configuration management tools such as Ansible.
Relevant experience with configuration and implementation of IaaS, Infrastructure as code, AWS.

Experience required 7 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4261923661,https://www.linkedin.com/jobs/view/4261923661,Pending,Unknown,2025-07-10 10:41:31.884742,Found a Bad Word in About Job,"
About the job
Role Overview:
Join a US-based deep tech innovator using a large satellite constellation to deliver real-time Earth observation. This is a hands-on Cloud Security role, ideal for someone with experience implementing frameworks like NIST 800-171 across cloud-native AWS environments.
You'll own security design and implementation during a key compliance phase, collaborating with DevOps and Engineering to build scalable, secure cloud solutions in Kubernetes.

Key Responsibilities:
Implement and maintain NIST SP 800-171 compliance across AWS environments
Build automated detection and response tools (CloudTrail, GuardDuty, CloudWatch)
Support secure architecture design and DevOps integration
Maintain the cloud security risk register and guide policy development
Contribute to DevOps sprints from a security-first perspective

Requirements:

3+ years' experience in cloud security (AWS, Kubernetes, containerized apps)
Proven track record implementing frameworks like NIST, ISO 27001, HIPAA, or CMMC
Scripting for automation (Python, Bash)
Familiarity with AWS security tools (Security Hub, WAF, SIEM, etc.)
US Citizenship required

Preferred Certifications:

AWS Certified Security, CISSP, CISM, CKS

Contains bad word ""US Citizen"". Skipping this job!
",Skipped,Not Available
4207665205,https://www.linkedin.com/jobs/view/4207665205,Pending,Unknown,2025-07-10 10:42:17.764276,Required experience is high,"
About the job
Title: DevOps Engineer (Mid, Senior, or Principal Level)

Location: Deerfield, IL or OPO (Hybrid â€“ 3 days onsite per week; minimum 1 day in Deerfield required)

Duration & Type: Typically 3-4 month Contract-to-Hire (Must be eligible to convert without visa sponsorship)

Compensation: Competitive W2 Hourly Rate ($77-$96, dependent on level and experience), Access to Healthcare and Dental Insurance Plan of Choice. (Benefit Plans can be requested at time of submission to client)

Summary

Chamberlain Advisors is proactively building a pipeline of DevOps Engineers at the Mid, Senior, and Principal levels to support cloud engineering platforms for a Fortune 100 client in the healthcare and retail pharmacy sector. These roles will contribute to the delivery of modern DevOps solutions within Azure environments, leveraging Infrastructure as Code, CI/CD pipelines, and secure automation practices. We are accepting qualified candidates now to support anticipated hiring needs in Q2 & Q3.

What Youâ€™ll Be Accountable For

Design, build, and support scalable DevOps pipelines in Azure-based environments 
Automate infrastructure using Infrastructure as Code tools such as Bicep, Terraform, or Ansible 
Build and maintain CI/CD pipelines using Git-based version control systems 
Optimize performance and troubleshoot across Linux and Windows systems 
Implement DevSecOps best practices to ensure secure and compliant deployments 
Document and maintain environment standards, recovery plans, and usage guidelines 
Collaborate with engineering, security, and operations teams to support agile release workflows 
(Principal Level) Drive architectural decisions and lead platform optimization initiatives 
(Principal Level) Mentor junior engineers and set DevOps standards across teams 

What Qualifications You Need 

Bachelorâ€™s degree in Computer Science, Software Engineering, or related field 
Experience levels:
Mid-Level: 5+ years 
Senior-Level: 7â€“9 years 
Principal-Level: 10+ years 
Proven hands-on experience with:
Azure Cloud services and administration 
Infrastructure as Code (Bicep, Terraform, Ansible, etc.) 
CI/CD systems and Git version control 
Scripting languages (Python, PowerShell) 
Operating systems (Linux and Windows) 
Strong troubleshooting, automation, and performance tuning capabilities 
Effective communicator with the ability to work cross-functionally in fast-paced teams 
Preferred Qualifications

Experience with Helm, Open Policy Agent (OPA), and hardened deployment environments 
Familiarity with enterprise change management systems and cloud security posture management 
Background in regulated industries (e.g., healthcare, pharmacy, or finance) 

Note: This is a pipeline requisition intended to identify strong candidates ahead of future hiring. If you're interested in being considered for upcoming opportunities and are qualified based on the above, we encourage you to apply now to join our talent community.

About The Client

Our fortune 20 client is seeking innovative and intelligent individuals to join their team. Here is your opportunity to join one of the largest healthcare and retail pharmacy companies in the U.S, with more than 10 million customers, over 8,000 retail stores, and a presence in multiple countries. Our client is constantly creating groundbreaking ways to meet customer needs, improve their health, and be a force for good in the world. This is your chance to work in a truly supportive environment and be a part of a progressive organization dedicated to the well-being of their customers, team members, and communities.

Why Work with Chamberlain?

Chamberlain Advisors is a veteran-owned business that provides human capital solutions through contract, contract-to-hire, direct-hire, and retained-search opportunities. Chamberlain candidates benefit from our unique hiring and interviewing process which has been designed to increase the likelihood that they will be successful in their job searches. This is achieved through our 5-step recruitment process, ensuring a top-of-the-line candidate experience. Find out what makes us different; apply to Chamberlain today.

Equal Employment Opportunity

Chamberlain Advisors provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, Chamberlain Advisors complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.

Chamberlain Advisors expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Chamberlain Advisors' employees to perform their job duties may result in discipline up to and including discharge.

Experience required 10 > Current Experience 6. Skipping this job!
",Skipped,Not Available
4081879898,https://www.linkedin.com/jobs/view/4081879898,Previous resume,Unknown,2025-07-10 10:42:28.399897,Problem in Easy Applying,"Message: invalid session id
Stacktrace:
0   chromedriver                        0x0000000104f78e6c cxxbridge1$str$ptr + 2722840
1   chromedriver                        0x0000000104f70d74 cxxbridge1$str$ptr + 2689824
2   chromedriver                        0x0000000104ac2260 cxxbridge1$string$len + 90252
3   chromedriver                        0x0000000104afccf8 cxxbridge1$string$len + 330532
4   chromedriver                        0x0000000104b252e4 cxxbridge1$string$len + 495888
5   chromedriver                        0x0000000104b245ec cxxbridge1$string$len + 492568
6   chromedriver                        0x0000000104a913d8 chromedriver + 87000
7   chromedriver                        0x0000000104f3bf88 cxxbridge1$str$ptr + 2473268
8   chromedriver                        0x0000000104f3f1f4 cxxbridge1$str$ptr + 2486176
9   chromedriver                        0x0000000104f1d9d0 cxxbridge1$str$ptr + 2348924
10  chromedriver                        0x0000000104f3fab0 cxxbridge1$str$ptr + 2488412
11  chromedriver                        0x0000000104f0ea60 cxxbridge1$str$ptr + 2287628
12  chromedriver                        0x0000000104a8f664 chromedriver + 79460
13  dyld                                0x000000019441ab98 start + 6076
",Easy Applied,Not Available
